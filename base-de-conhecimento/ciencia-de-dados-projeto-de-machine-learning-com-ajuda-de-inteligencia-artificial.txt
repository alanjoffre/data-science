Automatize o aprendizado de máquina com o ChatGPT

Projeto: https://www.youtube.com/watch?v=OmQiLvnY3WY&t=57s
Site do projeto: https://docs.datalumina.io/bJhyeJ4lHGfTl6
Dataset: https://capitalbikeshare.com/system-data

ETAPAS
----------
1 - Entendimento do negócio
2 - Entendimento de dados
3 - Preparação de dados
4 - Análise exploratória de dados
5 - Modelagem preditiva
6 - Avaliação do modelo
7 - Implantação do modelo
Volta para o passo: 1 (loop) 
----------

MACHINE LEARNIG:

#PROMPT1
Uma empresa que aluga bicicletas quer usar um modelo para prever o número de bicicletas que serão alugadas em um determinado dia. 
O modelo levará em conta vários fatores, como a data (mês, dia, se é feriado ou fim de semana), bem como dados climáticos (temperatura, precipitação, etc.) 
para fazer previsões precisas sobre a demanda de aluguel de bicicletas. Isso ajudará a empresa a planejar melhor a equipe e o inventário de bicicletas e otimizar a receita. 
Meu conjunto de dados está localizado em "D:\Github\data-science\projetos\daily-bike-shares\data\raw\daily-bike-share.csv". Você pode escrever um python para mim que possa ler este csv usando Pandas em um dataframe?


#PASSO1/COM RESPOSTA CHATGPT
import pandas as pd

# Caminho para o arquivo CSV
# Altere a direção da barra de "\" para "/"
file_path = r"D:/Github/data-science/projetos/daily-bike-shares/data/raw/daily-bike-share.csv"


# Carregando o CSV em um DataFrame
df = pd.read_csv(file_path)

# Exibindo as primeiras linhas do DataFrame para verificar se a leitura foi bem-sucedida
print(df.head())

#PASSO2
#COPIE OS NOMES DAS COLUNAS
df.comumns

#PROMPT2
você pode explicar esse conjunto de dados e como podemos criar um modelo de previsão para o aluguel? Estas são as colunas no conjunto de dados:'instant', 'dteday', 'season', 'yr', 'mnth', 'holiday', 'weekday',
       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed',
       'rentals'
	   
#PASSO3/RESPOSTA CHATGPT	   
- Vamos explorar cada coluna do seu conjunto de dados e pensar em como podemos usá-las para criar um modelo de previsão de demanda de aluguel de bicicletas.

- Colunas do Conjunto de Dados:
1 - instant: Um identificador único para cada registro no conjunto de dados (provavelmente uma chave primária). Não contribui para a previsão.
2 - dteday: A data específica (no formato de string ou datetime). Pode ser usada para extrair informações adicionais, como mês, ano, dia da semana, ou agrupar os dados por sazonalidade.
3 - season: Indica a estação do ano (1: inverno, 2: primavera, 3: verão, 4: outono). Pode ser uma variável categórica importante para capturar a sazonalidade.
4 - yr: O ano (0: 2011, 1: 2012). Captura as mudanças entre os dois anos.
5 - mnth: O mês (1 a 12). Captura variações sazonais ao longo do ano, que podem influenciar na demanda.
6 - holiday: Indica se o dia é um feriado (0: não, 1: sim). A demanda de bicicletas pode mudar em feriados, então essa coluna é relevante.
7 - weekday: O dia da semana (0: domingo, 1: segunda-feira, ..., 6: sábado). Pode ajudar a identificar padrões de demanda com base nos dias da semana.
8 - workingday: Indica se é um dia útil (0: fim de semana ou feriado, 1: dia útil). A demanda pode variar entre dias úteis e finais de semana.
9 - weathersit: A situação do clima (1: bom, 2: médio, 3: ruim, 4: muito ruim). O clima afeta diretamente o uso de bicicletas.
10 - temp: Temperatura normalizada (valores entre 0 e 1). Importante para capturar como o clima impacta a demanda.
11 - atemp: Temperatura "sensível" ou aparente, também normalizada. Pode ser mais representativa da percepção do usuário e, portanto, influenciar mais a demanda.
12 - hum: Umidade normalizada (valores entre 0 e 1). Clima úmido pode desincentivar o uso de bicicletas.
13 - windspeed: Velocidade do vento normalizada (valores entre 0 e 1). Velocidade alta de vento pode impactar negativamente o uso de bicicletas.
14 - rentals: A variável alvo que representa o número de bicicletas alugadas em um determinado dia. É isso que queremos prever com o nosso modelo.

>>>>>   Planejando a Criação do Modelo   <<<<<
 - Agora que temos uma ideia das variáveis, vamos pensar em como construir o modelo:

1 - Pré-processamento dos dados:
- Remover ou ajustar colunas que não são úteis para a previsão, como instant.
- Verificar se há valores faltantes (missing values) e tratá-los adequadamente.
- Converter variáveis categóricas como season, weathersit, weekday, mnth em variáveis dummies (codificação one-hot) para que possam ser interpretadas por algoritmos de machine learning.
- Normalizar ou padronizar as variáveis numéricas, como temp, atemp, hum e windspeed, se necessário.

2 - Divisão dos dados:
- Separar os dados em conjuntos de treinamento e teste. Geralmente, uma divisão de 80% para treinamento e 20% para teste é comum.
- Além disso, podemos usar uma divisão temporal, já que os dados são temporais, usando o ano de 2011 para treinamento e 2012 para teste, por exemplo.

3 - Seleção do modelo:

- Começar com modelos simples como Regressão Linear para prever a variável rentals.
- Testar outros modelos, como Árvore de Decisão, Random Forest, ou XGBoost, que são mais robustos para capturar relações não lineares e interações entre variáveis.

4 - Treinamento e avaliação:

- Treinar o modelo nos dados de treinamento.
- Avaliar o desempenho usando métricas como MAE (Mean Absolute Error), RMSE (Root Mean Squared Error) e R² (coeficiente de determinação).

5 - Ajuste e otimização:

- Ajustar hiperparâmetros dos modelos (se necessário) usando técnicas como Grid Search ou Random Search.
- Usar validação cruzada para evitar overfitting.

Esse pipeline básico deve ajudar a empresa a prever com precisão o número de bicicletas que serão alugadas em um determinado dia, com base nos fatores sazonais e climáticos.

----------

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# 1. Carregar o CSV
file_path = r"D:/Github/data-science/projetos/daily-bike-shares/data/raw/daily-bike-share.csv"
df = pd.read_csv(file_path)

# 2. Pré-processamento dos dados
# Remover colunas irrelevantes
df = df.drop(columns=['instant', 'dteday'])

# Separar a variável target ('rentals') das features
X = df.drop(columns=['rentals'])  # Features
y = df['rentals']                 # Target

# Criar variáveis dummies para variáveis categóricas (One-Hot Encoding)
X = pd.get_dummies(X, columns=['season', 'weathersit', 'mnth', 'weekday'], drop_first=True)

# Divisão em dados de treinamento e teste (80% treino, 20% teste)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. Treinamento do modelo (Random Forest)
rf_model = RandomForestRegressor(random_state=42)

# Definir parâmetros para GridSearch
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Grid Search para ajuste de hiperparâmetros
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Melhor modelo após o ajuste de hiperparâmetros
best_rf_model = grid_search.best_estimator_

# 4. Previsões nos dados de teste
y_pred = best_rf_model.predict(X_test)

# 5. Avaliação do modelo
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"MAE (Erro Médio Absoluto): {mae}")
print(f"RMSE (Raiz do Erro Quadrático Médio): {rmse}")
print(f"R² (Coeficiente de Determinação): {r2}")

# 6. Resultados da busca de parâmetros
print("Melhores parâmetros encontrados pelo Grid Search:", grid_search.best_params_)

----------

#PROMPT3
Os modelos de regressão são muito simples. Você pode me fornecer 5 algoritmos e código para treiná-los, avaliá-los e compará-los? Gere o código completo em uma única vez.

----------

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Algoritmos de regressão
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

# 1. Carregar o CSV e pré-processar os dados
file_path = r"D:/Github/data-science/projetos/daily-bike-shares/data/raw/daily-bike-share.csv"
df = pd.read_csv(file_path)

# Remover colunas irrelevantes
df = df.drop(columns=['instant', 'dteday'])

# Separar a variável target ('rentals') das features
X = df.drop(columns=['rentals'])  # Features
y = df['rentals']                 # Target

# Criar variáveis dummies para variáveis categóricas (One-Hot Encoding)
X = pd.get_dummies(X, columns=['season', 'weathersit', 'mnth', 'weekday'], drop_first=True)

# Dividir os dados em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 2. Função de avaliação
def evaluate_model(y_true, y_pred, model_name):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    
    print(f"Modelo: {model_name}")
    print(f"MAE (Erro Médio Absoluto): {mae}")
    print(f"RMSE (Raiz do Erro Quadrático Médio): {rmse}")
    print(f"R² (Coeficiente de Determinação): {r2}")
    print("-" * 40)
    
    return {'Model': model_name, 'MAE': mae, 'RMSE': rmse, 'R2': r2}

# 3. Lista para armazenar os resultados
results = []

# 4. Treinar e avaliar os modelos

# 4.1 Regressão Linear
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_test)
results.append(evaluate_model(y_test, y_pred_lr, "Regressão Linear"))

# 4.2 Árvore de Decisão
dt_model = DecisionTreeRegressor(random_state=42)
dt_model.fit(X_train, y_train)
y_pred_dt = dt_model.predict(X_test)
results.append(evaluate_model(y_test, y_pred_dt, "Árvore de Decisão"))

# 4.3 Random Forest
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)
results.append(evaluate_model(y_test, y_pred_rf, "Random Forest"))

# 4.4 XGBoost
xgb_model = XGBRegressor(random_state=42)
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)
results.append(evaluate_model(y_test, y_pred_xgb, "XGBoost"))

# 4.5 LightGBM
lgb_model = LGBMRegressor(random_state=42)
lgb_model.fit(X_train, y_train)
y_pred_lgb = lgb_model.predict(X_test)
results.append(evaluate_model(y_test, y_pred_lgb, "LightGBM"))

# 5. Comparar os Modelos

# Converter os resultados em um DataFrame
results_df = pd.DataFrame(results)

# Exibir os resultados ordenados pelo RMSE
print("Comparação dos Modelos:")
print(results_df.sort_values(by='RMSE'))

----------

#PROMPT4
Você pode combinar todo o código em uma função que testa todos os algoritmos e salva as pontuações para escolher o melhor no final?

----------

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

def test_models(file_path):
    # 1. Carregar o CSV e pré-processar os dados
    df = pd.read_csv(file_path)
    
    # Remover colunas irrelevantes
    df = df.drop(columns=['instant', 'dteday'])
    
    # Separar a variável target ('rentals') das features
    X = df.drop(columns=['rentals'])  # Features
    y = df['rentals']                 # Target

    # Criar variáveis dummies para variáveis categóricas (One-Hot Encoding)
    X = pd.get_dummies(X, columns=['season', 'weathersit', 'mnth', 'weekday'], drop_first=True)

    # Dividir os dados em treino e teste
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 2. Função de avaliação
    def evaluate_model(y_true, y_pred):
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        r2 = r2_score(y_true, y_pred)
        return mae, rmse, r2

    # 3. Lista para armazenar os resultados
    results = []

    # 4. Definir e testar os modelos
    models = {
        "Regressão Linear": LinearRegression(),
        "Árvore de Decisão": DecisionTreeRegressor(random_state=42),
        "Random Forest": RandomForestRegressor(random_state=42),
        "XGBoost": XGBRegressor(random_state=42),
        "LightGBM": LGBMRegressor(random_state=42)
    }

    for model_name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        mae, rmse, r2 = evaluate_model(y_test, y_pred)
        results.append({'Model': model_name, 'MAE': mae, 'RMSE': rmse, 'R2': r2})

    # 5. Comparar os Modelos
    results_df = pd.DataFrame(results)
    best_model = results_df.loc[results_df['RMSE'].idxmin()]

    # Exibir resultados e o melhor modelo
    print("Comparação dos Modelos:")
    print(results_df.sort_values(by='RMSE'))
    print("\nMelhor Modelo:")
    print(best_model)

# Exemplo de uso da função
file_path = r"D:/Github/data-science/projetos/daily-bike-shares/data/raw/daily-bike-share.csv"
test_models(file_path)

----------

#PROMPT5
O XGboost é o melhor modelo. Você pode criar uma função para executar uma pesquisa de grade sobre os hiperparâmetros mais importantes para ajustar o modelo? 

----------

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error
from xgboost import XGBRegressor

def optimize_xgboost(file_path):
    # 1. Carregar o CSV e pré-processar os dados
    df = pd.read_csv(file_path)
    
    # Remover colunas irrelevantes
    df = df.drop(columns=['instant', 'dteday'])
    
    # Separar a variável target ('rentals') das features
    X = df.drop(columns=['rentals'])  # Features
    y = df['rentals']                 # Target

    # Criar variáveis dummies para variáveis categóricas (One-Hot Encoding)
    X = pd.get_dummies(X, columns=['season', 'weathersit', 'mnth', 'weekday'], drop_first=True)

    # Dividir os dados em treino e teste
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 2. Definindo o modelo XGBoost
    xgb_model = XGBRegressor(random_state=42)

    # 3. Definindo os hiperparâmetros para a pesquisa de grade
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 5, 7],
        'learning_rate': [0.01, 0.05, 0.1],
        'subsample': [0.7, 0.8, 1.0]
    }

    # 4. Configurando a pesquisa de grade
    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, 
                               scoring='neg_mean_squared_error', cv=3, verbose=1, n_jobs=-1)

    # 5. Executando a pesquisa de grade
    grid_search.fit(X_train, y_train)

    # 6. Resultados da pesquisa de grade
    best_params = grid_search.best_params_
    best_rmse = (abs(grid_search.best_score_)) ** 0.5

    print("Melhores Hiperparâmetros encontrados:")
    print(best_params)
    print(f"Melhor RMSE: {best_rmse}")

# Exemplo de uso da função
file_path = r"D:/Github/data-science/projetos/daily-bike-shares/data/raw/daily-bike-share.csv"
optimize_xgboost(file_path)

----------

#PROMPT6
Melhores Hiperparâmetros encontrados:
{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 300, 'subsample': 0.8}
Melhor RMSE: 283.63789304058815
Gere o novo código com os hiperparamentros.

----------

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from xgboost import XGBRegressor
import numpy as np

def evaluate_model(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    return mae, rmse, r2

def optimize_xgboost(file_path):
    # 1. Carregar o CSV e pré-processar os dados
    df = pd.read_csv(file_path)
    
    # Remover colunas irrelevantes
    df = df.drop(columns=['instant', 'dteday'])
    
    # Separar a variável target ('rentals') das features
    X = df.drop(columns=['rentals'])  # Features
    y = df['rentals']                 # Target

    # Criar variáveis dummies para variáveis categóricas (One-Hot Encoding)
    X = pd.get_dummies(X, columns=['season', 'weathersit', 'mnth', 'weekday'], drop_first=True)

    # Dividir os dados em treino e teste
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 2. Definindo o modelo XGBoost
    xgb_model = XGBRegressor(random_state=42)

    # 3. Definindo os hiperparâmetros para a pesquisa de grade
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 5, 7],
        'learning_rate': [0.01, 0.05, 0.1],
        'subsample': [0.7, 0.8, 1.0]
    }

    # 4. Configurando a pesquisa de grade
    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, 
                               scoring='neg_mean_squared_error', cv=3, verbose=1, n_jobs=-1)

    # 5. Executando a pesquisa de grade
    grid_search.fit(X_train, y_train)

    # 6. Resultados da pesquisa de grade
    best_params = grid_search.best_params_
    best_rmse = (abs(grid_search.best_score_)) ** 0.5

    print("Melhores Hiperparâmetros encontrados:")
    print(best_params)
    print(f"Melhor RMSE: {best_rmse}")

    return best_params

def train_final_model(file_path, best_params):
    # Carregar e pré-processar os dados
    df = pd.read_csv(file_path)
    df = df.drop(columns=['instant', 'dteday'])
    
    X = df.drop(columns=['rentals'])
    y = df['rentals']
    X = pd.get_dummies(X, columns=['season', 'weathersit', 'mnth', 'weekday'], drop_first=True)

    # Dividir os dados
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Treinar o modelo com os melhores hiperparâmetros
    final_model = XGBRegressor(
        learning_rate=best_params['learning_rate'],
        max_depth=best_params['max_depth'],
        n_estimators=best_params['n_estimators'],
        subsample=best_params['subsample'],
        random_state=42
    )
    
    final_model.fit(X_train, y_train)

    # Prever e avaliar
    y_pred = final_model.predict(X_test)
    mae, rmse, r2 = evaluate_model(y_test, y_pred)

    print("\nModelo Final Avaliado:")
    print(f"MAE: {mae}, RMSE: {rmse}, R²: {r2}")

# Exemplo de uso das funções
file_path = r"D:/Github/data-science/projetos/daily-bike-shares/data/raw/daily-bike-share.csv"
best_params = optimize_xgboost(file_path)
train_final_model(file_path, best_params)

----------

#PROMPT7
você pode criar previsões para y e criar vários gráficos para inspecionar visualmente os resultados?

----------

Resposta: 
Claro! Podemos criar previsões com o modelo final treinado e, em seguida, gerar gráficos para inspecionar visualmente os resultados. 
Vamos adicionar as seguintes etapas ao código:
Fazer Previsões: Usar o modelo treinado para prever os valores de y (número de bicicletas alugadas).
Visualização dos Resultados: Usar a biblioteca matplotlib para criar gráficos que mostrem:
Comparação entre os valores reais e previstos.
Distribuição dos erros de previsão.

----------

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from xgboost import XGBRegressor
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def evaluate_model(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    return mae, rmse, r2

def optimize_xgboost(file_path):
    df = pd.read_csv(file_path)
    df = df.drop(columns=['instant', 'dteday'])

    # Feature Engineering: criar novas variáveis
    df['temp_hum'] = df['temp'] * df['hum']  # Interação entre temperatura e umidade
    df['temp_squared'] = df['temp'] ** 2  # Temperatura ao quadrado
    df['weekday_weekend'] = df['workingday'].apply(lambda x: 1 if x == 0 else 0)  # 1 para fim de semana, 0 para dia útil

    X = df.drop(columns=['rentals'])
    y = df['rentals']
    X = pd.get_dummies(X, columns=['season', 'weathersit', 'mnth', 'weekday'], drop_first=True)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    xgb_model = XGBRegressor(random_state=42)

    # Ajuste do espaço de busca dos hiperparâmetros
    param_grid = {
        'n_estimators': [300, 400, 500],
        'max_depth': [3, 4, 5, 6],
        'learning_rate': [0.01, 0.05, 0.1],
        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0]
    }

    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, 
                               scoring='neg_mean_squared_error', cv=3, verbose=1, n_jobs=-1)

    grid_search.fit(X_train, y_train)
    best_params = grid_search.best_params_
    best_rmse = (abs(grid_search.best_score_)) ** 0.5

    print("Melhores Hiperparâmetros encontrados:")
    print(best_params)
    print(f"Melhor RMSE: {best_rmse}")

    return best_params, X_test, y_test

def train_final_model(file_path, best_params):
    df = pd.read_csv(file_path)
    df = df.drop(columns=['instant', 'dteday'])

    # Feature Engineering: aplicar as mesmas transformações
    df['temp_hum'] = df['temp'] * df['hum']
    df['temp_squared'] = df['temp'] ** 2
    df['weekday_weekend'] = df['workingday'].apply(lambda x: 1 if x == 0 else 0)

    X = df.drop(columns=['rentals'])
    y = df['rentals']
    X = pd.get_dummies(X, columns=['season', 'weathersit', 'mnth', 'weekday'], drop_first=True)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    final_model = XGBRegressor(
        learning_rate=best_params['learning_rate'],
        max_depth=best_params['max_depth'],
        n_estimators=best_params['n_estimators'],
        subsample=best_params['subsample'],
        random_state=42
    )
    
    final_model.fit(X_train, y_train)

    # Validação cruzada para obter uma avaliação mais robusta
    cv_scores = cross_val_score(final_model, X, y, cv=5, scoring='neg_mean_squared_error')
    print(f'Média RMSE com validação cruzada: {np.sqrt(-cv_scores.mean())}')

    y_pred = final_model.predict(X_test)
    mae, rmse, r2 = evaluate_model(y_test, y_pred)

    print("\nModelo Final Avaliado:")
    print(f"MAE: {mae}, RMSE: {rmse}, R²: {r2}")

    return y_test, y_pred

def plot_results(y_test, y_pred):
    plt.figure(figsize=(14, 6))

    # Gráfico de comparação entre valores reais e previstos
    plt.subplot(1, 2, 1)
    plt.scatter(y_test, y_pred, alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.title('Valores Reais vs. Valores Previsto')
    plt.xlabel('Valores Reais')
    plt.ylabel('Valores Previsto')
    plt.xlim(y_test.min(), y_test.max())
    plt.ylim(y_test.min(), y_test.max())

    # Gráfico da distribuição dos erros
    plt.subplot(1, 2, 2)
    errors = y_test - y_pred
    sns.histplot(errors, kde=True, bins=30)
    plt.title('Distribuição dos Erros de Previsão')
    plt.xlabel('Erro (Valores Reais - Valores Previsto)')
    plt.ylabel('Frequência')

    plt.tight_layout()
    plt.show()

# Exemplo de uso das funções
file_path = r"D:/Github/data-science/projetos/daily-bike-shares/data/raw/daily-bike-share.csv"
best_params, X_test, y_test = optimize_xgboost(file_path)
y_test, y_pred = train_final_model(file_path, best_params)
plot_results(y_test, y_pred)

----------

#PROMPT8
como posso exportar o modelo para "D:\Github\data-science\projetos\daily-bike-shares\src\models" e depois colocá-lo em produção para que meu cliente possa começar a usar o modelo para fazer previsões?

----------

Exportar um modelo treinado para produção é uma etapa importante no fluxo de trabalho de ciência de dados. Vou guiá-lo pelas etapas necessárias para salvar o modelo treinado e colocá-lo em produção.

Etapas para Exportar e Colocar o Modelo em Produção
1 - Exportar o Modelo Treinado: Usaremos a biblioteca joblib para salvar o modelo. Isso é eficiente para modelos de machine learning e permite a fácil recuperação do modelo posteriormente.
2 - Criar um Script para Carregar e Fazer Previsões: Um script separado pode ser criado para carregar o modelo e fazer previsões com novos dados.
3 - Criar uma API (opcional): Se você deseja que seu cliente faça previsões a partir de um aplicativo, pode criar uma API usando um framework como Flask ou FastAPI.

1. Exportar o Modelo Treinado
Adicione a seguinte parte ao final da sua função train_final_model:

----------

import joblib

# Salvar o modelo
model_path = r"D:\Github\data-science\projetos\daily-bike-shares\src\models\xgb_model.joblib"
joblib.dump(final_model, model_path)
print(f"Modelo salvo em: {model_path}")

----------

2. Criar um Script para Carregar e Fazer Previsões
Crie um novo arquivo Python, por exemplo, predict.py, onde você irá carregar o modelo e fazer previsões:

----------

import joblib
import pandas as pd
import numpy as np

# Carregar o modelo
model_path = r"D:\Github\data-science\projetos\daily-bike-shares\src\models\xgb_model.joblib"
model = joblib.load(model_path)

# Função para fazer previsões
def make_prediction(input_data):
    # Processar os dados de entrada (certifique-se de que as features correspondem ao modelo)
    input_df = pd.DataFrame(input_data)
    
    # Realizar qualquer transformação necessária, como dummy variables
    input_df['temp_hum'] = input_df['temp'] * input_df['hum']
    input_df['temp_squared'] = input_df['temp'] ** 2
    input_df['weekday_weekend'] = input_df['workingday'].apply(lambda x: 1 if x == 0 else 0)
    input_df = pd.get_dummies(input_df, columns=['season', 'weathersit', 'mnth', 'weekday'], drop_first=True)

    # Fazer a previsão
    prediction = model.predict(input_df)
    return prediction

# Exemplo de uso
input_data = {
    'temp': [0.5],
    'hum': [0.3],
    'windspeed': [0.1],
    'season': [1],
    'weathersit': [1],
    'mnth': [4],
    'weekday': [5],
    'workingday': [1]
}
pred = make_prediction(input_data)
print(f"Previsão de aluguel de bicicletas: {pred}")

----------

3. Criar uma API (Opcional)
Se o seu cliente desejar um método mais interativo para fazer previsões, você pode criar uma API simples. Aqui está um exemplo usando Flask:


from flask import Flask, request, jsonify
import joblib
import pandas as pd

app = Flask(__name__)

# Carregar o modelo
model_path = r"D:\Github\data-science\projetos\daily-bike-shares\src\models\xgb_model.joblib"
model = joblib.load(model_path)

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json(force=True)
    input_df = pd.DataFrame(data)
    
    # Processar os dados
    input_df['temp_hum'] = input_df['temp'] * input_df['hum']
    input_df['temp_squared'] = input_df['temp'] ** 2
    input_df['weekday_weekend'] = input_df['workingday'].apply(lambda x: 1 if x == 0 else 0)
    input_df = pd.get_dummies(input_df, columns=['season', 'weathersit', 'mnth', 'weekday'], drop_first=True)
    
    # Fazer a previsão
    prediction = model.predict(input_df)
    return jsonify({'predictions': prediction.tolist()})

if __name__ == '__main__':
    app.run(debug=True)
	
----------
	
Colocando em Produção
Serviço de API: Você pode usar um servidor de produção como gunicorn para servir a API Flask.
Hospedagem: Coloque o script do Flask em um servidor (como AWS, Heroku ou um servidor local).
Fazer Chamadas à API: Seu cliente pode fazer requisições POST à rota /predict passando os dados necessários para obter previsões.

Conclusão
Com essas etapas, você terá um modelo que pode ser facilmente carregado e usado para previsões, e, opcionalmente, uma API que pode ser acessada por seu cliente para fazer previsões em tempo real. Se precisar de mais ajuda em alguma das etapas, sinta-se à vontade para perguntar!

----------

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from xgboost import XGBRegressor
import numpy as np
import joblib
import matplotlib.pyplot as plt
import seaborn as sns

def evaluate_model(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    return mae, rmse, r2

def optimize_xgboost(file_path):
    df = pd.read_csv(file_path)
    df = df.drop(columns=['instant', 'dteday'])

    # Feature Engineering: criar novas variáveis
    df['temp_hum'] = df['temp'] * df['hum']  # Interação entre temperatura e umidade
    df['temp_squared'] = df['temp'] ** 2  # Temperatura ao quadrado
    df['weekday_weekend'] = df['workingday'].apply(lambda x: 1 if x == 0 else 0)  # 1 para fim de semana, 0 para dia útil

    X = df.drop(columns=['rentals'])
    y = df['rentals']
    X = pd.get_dummies(X, columns=['season', 'weathersit', 'mnth', 'weekday'], drop_first=True)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    xgb_model = XGBRegressor(random_state=42)

    # Ajuste do espaço de busca dos hiperparâmetros
    param_grid = {
        'n_estimators': [300, 400, 500],
        'max_depth': [3, 4, 5, 6],
        'learning_rate': [0.01, 0.05, 0.1],
        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0]
    }

    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, 
                               scoring='neg_mean_squared_error', cv=3, verbose=1, n_jobs=-1)

    grid_search.fit(X_train, y_train)
    best_params = grid_search.best_params_
    best_rmse = (abs(grid_search.best_score_)) ** 0.5

    print("Melhores Hiperparâmetros encontrados:")
    print(best_params)
    print(f"Melhor RMSE: {best_rmse}")

    return best_params, X_test, y_test

def train_final_model(file_path, best_params):
    df = pd.read_csv(file_path)
    df = df.drop(columns=['instant', 'dteday'])

    # Feature Engineering: aplicar as mesmas transformações
    df['temp_hum'] = df['temp'] * df['hum']
    df['temp_squared'] = df['temp'] ** 2
    df['weekday_weekend'] = df['workingday'].apply(lambda x: 1 if x == 0 else 0)

    X = df.drop(columns=['rentals'])
    y = df['rentals']
    X = pd.get_dummies(X, columns=['season', 'weathersit', 'mnth', 'weekday'], drop_first=True)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    final_model = XGBRegressor(
        learning_rate=best_params['learning_rate'],
        max_depth=best_params['max_depth'],
        n_estimators=best_params['n_estimators'],
        subsample=best_params['subsample'],
        random_state=42
    )
    
    final_model.fit(X_train, y_train)

    # Salvar o modelo
    model_path = r"D:/Github/data-science/projetos/daily-bike-shares/src/models/xgb_model.joblib"
    joblib.dump(final_model, model_path)
    print(f"Modelo salvo em: {model_path}")

    # Validação cruzada para obter uma avaliação mais robusta
    cv_scores = cross_val_score(final_model, X, y, cv=5, scoring='neg_mean_squared_error')
    print(f'Média RMSE com validação cruzada: {np.sqrt(-cv_scores.mean())}')

    y_pred = final_model.predict(X_test)
    mae, rmse, r2 = evaluate_model(y_test, y_pred)

    print("\nModelo Final Avaliado:")
    print(f"MAE: {mae}, RMSE: {rmse}, R²: {r2}")

    return y_test, y_pred

def plot_results(y_test, y_pred):
    plt.figure(figsize=(14, 6))

    # Gráfico de comparação entre valores reais e previstos
    plt.subplot(1, 2, 1)
    plt.scatter(y_test, y_pred, alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.title('Valores Reais vs. Valores Previsto')
    plt.xlabel('Valores Reais')
    plt.ylabel('Valores Previsto')
    plt.xlim(y_test.min(), y_test.max())
    plt.ylim(y_test.min(), y_test.max())

    # Gráfico da distribuição dos erros
    plt.subplot(1, 2, 2)
    errors = y_test - y_pred
    sns.histplot(errors, kde=True, bins=30)
    plt.title('Distribuição dos Erros de Previsão')
    plt.xlabel('Erro (Valores Reais - Valores Previsto)')
    plt.ylabel('Frequência')

    plt.tight_layout()
    plt.show()

# Exemplo de uso das funções
file_path = r"D:/Github/data-science/projetos/daily-bike-shares/data/raw/daily-bike-share.csv"
best_params, X_test, y_test = optimize_xgboost(file_path)
y_test, y_pred = train_final_model(file_path, best_params)
plot_results(y_test, y_pred)


model_path = r"D:/Github/data-science/projetos/daily-bike-shares/src/models/xgb_model.joblib"

----------

Código: make_dataset.py

import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Função para carregar e pré-processar os dados
def load_and_preprocess_data(file_path):
    # Carregar dados
    data = pd.read_csv(file_path)

    # Criar novas variáveis
    data['temp_hum'] = data['temp'] * data['hum']
    data['temp_squared'] = data['temp'] ** 2
    data['weekday_weekend'] = data['workingday'].apply(lambda x: 1 if x == 0 else 0)

    # Codificação one-hot para as variáveis categóricas
    data = pd.get_dummies(data, columns=['season', 'weathersit', 'mnth', 'weekday'], drop_first=True)

    return data

# Função para dividir dados e treinar o modelo
def train_model(data):
    X = data.drop(columns=['instant', 'dteday', 'rentals'])  # Features
    y = data['rentals']  # Target

    # Dividir os dados
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Treinamento do modelo
    model = XGBRegressor(objective='reg:squarederror')
    model.fit(X_train, y_train)

    # Previsões e avaliação
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    print(f'MAE: {mae}, RMSE: {rmse}, R²: {r2}')
    
    return model, X_train.columns

# Função para otimizar hiperparâmetros usando GridSearchCV
def optimize_xgboost(X_train, y_train):
    param_grid = {
        'learning_rate': [0.01, 0.05, 0.1],
        'max_depth': [3, 5, 7],
        'n_estimators': [100, 300],
        'subsample': [0.8, 1.0],
    }

    grid_search = GridSearchCV(XGBRegressor(objective='reg:squarederror'), param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)
    grid_search.fit(X_train, y_train)

    print("Melhores Hiperparâmetros encontrados:", grid_search.best_params_)
    print("Melhor RMSE:", np.sqrt(-grid_search.best_score_))

    return grid_search.best_estimator_

# Função para plotar resultados
def plot_results(y_test, y_pred):
    plt.figure(figsize=(10, 5))
    sns.scatterplot(x=y_test, y=y_pred)
    plt.xlabel("Valores Reais")
    plt.ylabel("Previsões")
    plt.title("Valores Reais vs Previsões")
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
    plt.show()

    # Histogramas de erros
    errors = y_pred - y_test
    plt.figure(figsize=(10, 5))
    sns.histplot(errors, bins=30, kde=True)
    plt.title("Distribuição dos Erros de Previsão")
    plt.xlabel("Erro")
    plt.ylabel("Frequência")
    plt.show()

# Função para pré-processar dados de entrada para previsão
def preprocess_input(input_data, model_feature_names):
    input_data['temp_hum'] = input_data['temp'] * input_data['hum']
    input_data['temp_squared'] = input_data['temp'] ** 2
    input_data['weekday_weekend'] = input_data['workingday'].apply(lambda x: 1 if x == 0 else 0)

    # Codificação one-hot para as variáveis categóricas
    input_data = pd.get_dummies(input_data, columns=['season', 'weathersit', 'mnth', 'weekday'], drop_first=True)

    # Garantir que todas as colunas que o modelo espera estão presentes, mesmo que ausentes
    input_data = input_data.reindex(columns=model_feature_names, fill_value=0)

    return input_data

# Função para fazer previsões
def make_prediction(input_data):
    model_path = r"D:\Github\data-science\projetos\daily-bike-shares\src\models\xgb_model.joblib"
    model = joblib.load(model_path)

    # Preprocessar os dados de entrada
    input_df = preprocess_input(input_data, model.get_booster().feature_names)

    # Prever
    prediction = model.predict(input_df)
    return prediction

# Função principal
def main():
    file_path = r"D:\Github\data-science\projetos\daily-bike-shares\data\raw\daily-bike-share.csv"
    data = load_and_preprocess_data(file_path)

    # Treinamento do modelo
    model, feature_names = train_model(data)

    # Otimização de hiperparâmetros
    best_model = optimize_xgboost(data[feature_names], data['rentals'])

    # Salvar o modelo otimizado
    joblib.dump(best_model, r"D:\Github\data-science\projetos\daily-bike-shares\src\models\xgb_model.joblib")

    # Avaliar o modelo otimizado
    y_test = data['rentals']
    y_pred = best_model.predict(data[feature_names])
    plot_results(y_test, y_pred)

# Exemplo de previsão
def predict_example():
    input_data = pd.DataFrame({
        'yr': [0],  # 0 para 2011, 1 para 2012
        'holiday': [0],  # 1 se feriado, 0 caso contrário
        'workingday': [1],  # 1 se dia útil, 0 caso contrário
        'temp': [0.5],  # Temperatura normalizada
        'atemp': [0.5],  # Temperatura ajustada normalizada
        'hum': [0.4],  # Umidade normalizada
        'windspeed': [0.1],  # Velocidade do vento normalizada
        'season': [1],  # 1- Inverno, 2- Primavera, 3- Verão, 4- Outono
        'weathersit': [1],  # 1- Limpo, 2- Nublado, 3- Chuva, 4- Neve
        'mnth': [1],  # 1- Janeiro, 2- Fevereiro, ..., 12- Dezembro
        'weekday': [0],  # 0- Domingo, 1- Segunda, ..., 6- Sábado
    })

    pred = make_prediction(input_data)
    print("Previsão de aluguel:", pred)

if __name__ == "__main__":
    main()
    predict_example()

----------

Código: predict.py

import joblib
import pandas as pd

# Carregar o modelo
model_path = r"D:/Github/data-science/projetos/daily-bike-shares/src/models/xgb_model.joblib"
model = joblib.load(model_path)

# Função para processar os dados de entrada
def preprocess_input(input_data):
    # Criar DataFrame a partir dos dados de entrada
    input_df = pd.DataFrame(input_data)

    # Realizar as transformações necessárias
    input_df['temp_hum'] = input_df['temp'] * input_df['hum']
    input_df['temp_squared'] = input_df['temp'] ** 2
    input_df['weekday_weekend'] = input_df['workingday'].apply(lambda x: 1 if x == 0 else 0)

    # Codificação one-hot para variáveis categóricas
    input_df = pd.get_dummies(input_df, columns=['season', 'weathersit', 'mnth', 'weekday'], drop_first=True)

    # Garantir que todas as colunas que o modelo espera estão presentes
    model_feature_names = model.get_booster().feature_names
    input_df = input_df.reindex(columns=model_feature_names, fill_value=0)

    return input_df

# Função para fazer previsões
def make_prediction(input_data):
    # Processar os dados de entrada
    input_df = preprocess_input(input_data)

    # Fazer a previsão
    prediction = model.predict(input_df)
    return prediction

# Exemplo de uso
input_data = {
    'temp': [0.5],
    'hum': [0.3],
    'windspeed': [0.1],
    'season': [1],  # 1- Inverno, 2- Primavera, 3- Verão, 4- Outono
    'weathersit': [1],  # 1- Limpo, 2- Nublado, 3- Chuva, 4- Neve
    'mnth': [4],  # 1- Janeiro, 2- Fevereiro, ..., 12- Dezembro
    'weekday': [5],  # 0- Domingo, 1- Segunda, ..., 6- Sábado
    'workingday': [1]  # 1 se dia útil, 0 caso contrário
}

pred = make_prediction(input_data)
print(f"Previsão de aluguel de bicicletas: {pred[0]}")

----------


