VISÃO GERAL DO PROJETO - ANALISE DE CREDITO - CIENCIA DE DADOS – 2024
RESUMO

EMPRESA: NOVADRIVE
MONTADORA DE VEICULOS
Site da empresa NOVADRIVE: URL=https://www.novadrivemotors.com.br/
Vaga disponível: Cientista de Dados: URL=https://www.novadrivemotors.com.br/vagas.html

TODAS AS ETAPAS DO PROJETO
1 - Necessidades do cliente
2 - Dados 
3 - EDA - Analise Exploratória de Dados
4 - Pré-processamento
5 - Criar modelo / Objetos
6 - XAI - Inteligência Artificial Explicável
7 - Criar API (Interface de Programação de Aplicativos) / Criar UI (Interface Usuário)
8 - Deploy
9 - Considerações Finais e possíveis melhorias

1 – Necessidades do cliente
- Sistema de concessão de crédito
- Avaliar a concessão de crédito
- Performance mínima: Precisão mínima 70%, Esperado 80%
- Recall mínimo 70%, Esperado 75%
- Limiar de decisão: 50%, ajustável
- Tecnica de Explicabilidade (XAI)
- Servida através de API
- Tela UI (Interface Usuário)
Infraestrutura:  Publicar no AWS - EC2 /  Informações para extrair os dados


2 - Dados
2.1 - Tabelas: Clientes, Pedido de crédito, Parcelas Crédito, Produtos financiados

2.2 - Informações das tabelas:
- Clientes: ClienteID, Nome, CPF, Email, Profissao, TempoProfissao, Renda, TipoResidencia, Escolaridade, DataNascimento, Dependentes, EstadoCivil, Telefone, Score
- PedidoCredito: SolicitacaoID, ClienteID, ProdutoID, ValorSolicitado, ValorTOtalItem, NumeroParcelas, DataSolicitacao, Status
- ParcelasCredito: SolicitacaoID, NumeroParcelas, ValorParcela, DataVencimento, DataPagamento, Status
- ProdutosFincanciados: ProdutoID, NomeComercial, PrecoTabela

2.3 - Relacionamentos: 
Clientes/ClienteID -> PedidoCredito/ClienteID
PedidoCredito/SolicitacaoID -> ParcelasCredito/SolicitacaoID
PedidoCredito/ProdutoID -> ProdutosFinanciados/ProdutoID
 
2.4 - Identificar atributos com valores semânticos - Atributos importantes
- Tabela Clientes: Profissao, TempoProfissao, Renda, TipoResidencia, Escolaridade, DataNacimento, Dependentes, EstadoCivil, Score
- Tabela PedidoCredito: ValorSolicitado, ValorTotalBem
- Tabela ParcelasCredito: Status
- ProdutosFinaciados: NomeComercial (do produto)

2.5 - Cardinalidade - Categorias
Profissão: Cientista de Dados, Médico, Dentista, Contador...
TipoResidencia: Outros, Alugada, Própria,
Escolaridade: Ens. Fundamental, Ens. Médio, Pósou Mais, Superior
EstadoCivil: Divorciado, Casado, Solteiro, Viuvo
Score: Baixo, MuitoBom, Bom, Justo
Produtos: AgileXplorer, e outros

2.6 - Variáveis Numéricas
- Não há problema de cardinalidade
- Porém devemos considerar: Valores nulos e Outliers (valores discrepantes)

2.7 - Tabela Final: Modelo
Tabela Modelo: Profissao, TempoProfissao, Renda, TipoResidencia, Escolaridade, Score, Idade, Dependentes, EstadoCivil, Produto, ValorSolicitado, ValorTotalBem

2.8 - Acesso a base de dados
- Conexão remota via PGADMIN
- Credenciais do banco de dados

dbname: 'novadrivebank'
user: 'etlreandonlybank'
password: 'novadrive376A@'
host: '159.223.187..110'

- Logando no PGADMIN
- Acessando tabelas do banco de dados
- Acessando Query Tool
- Criando consulta SQL, para desnomalização dos dados

----------
SELECT c.Profissao,
       c.TempoProfissao,
       c.Renda,
       c.TipoResidencia,
       c.Escolaridade,
	   c.Score,
       EXTRACT(YEAR FROM AGE(c.DataNascimento)) AS Idade,
       c.Dependentes,
       c.EstadoCivil,
       pf.NomeComercial AS Produto,
       pc.ValorSolicitado,
       pc.ValorTotalBem,
       CASE 
           WHEN COUNT(p.Status) FILTER (WHERE p.Status = 'Vencido') > 0 THEN 'ruim'
           ELSE 'bom'
       END AS Classe
FROM clientes c
JOIN PedidoCredito pc ON c.ClienteID = pc.ClienteID
JOIN ProdutosFinanciados pf ON pc.ProdutoID = pf.ProdutoID
LEFT JOIN ParcelasCredito p ON pc.SolicitacaoID = p.SolicitacaoID
WHERE pc.Status = 'Aprovado'
GROUP BY c.ClienteID, pf.NomeComercial, pc.ValorSolicitado, pc.ValorTotalBem
----------
- Dados normalizados após a execução do SQL

3 - EDA – Analise Exploratória de Dados

3.1 - Ambiente: 
- Python3.7 ou superior
- VSCode
- Virtual Env

Windows
- python -m venv novadrivebank
- novadrivebank\Scripts\activate
- pip install -r requirements.txt
- deactivate


Mac/Linux
- python3 -m venv novadrivebank
- source novadrivebank\bin\activate
- pip install -r requirements.txt
- deactivate

----------
Conteúdo arquivo: .gitignore
Nota: O arquivo deve contar dentro do repositório de exploratória.
----------
config.yaml
meu_modelo.keras
objects/*
!objects/.gitkeep
.gitignore
__pycache__
novadrivebank
----------
//////////
----------
Conteúdo arquivo: requirements.txt
Nota: O arquivo deve contar dentro do repositório de exploratória.
----------
fuzzywuzzy==0.18.0
pandas==2.2.1
scikit-learn==1.4.1.post1
joblib==1.3.2
PyYAML==6.0.1
psycopg2-binary==2.9.9
numpy==1.26.4
tensorflow==2.16.1
shap==0.45.0
flask==3.0.3
python-Levenshtein==0.25.1
streamlit==1.32.1
matplotlib==3.8.3
seaborn==0.13.2
----------

O que cada biblioteca faz: 
fuzzywuzzy==0.18.0: Calcula a similaridade entre duas strings usando diferentes algoritmos, como Levenshtein distance e Jaro-Winkler distance.
pandas==2.2.1: Fornece estruturas de dados e ferramentas de análise para manipulação de dados eficiente em Python.
Scikit-learn==1.4.1.post1: Uma biblioteca de aprendizado de máquina com algoritmos para classificação, regressão, clustering e outras tarefas.
joblib==1.3.2: Uma biblioteca que permite a persistência de objetos Python e a execução de tarefas em paralelo.
PyYAML==6.0.1: Uma biblioteca para carregar e salvar dados em formato YAML.
psycopg2-binary==2.9.9: Um adaptador para o banco de dados PostgreSQL.
numpy==1.26.4: Uma biblioteca para computação numérica em Python, com suporte para arrays e matrizes multidimensionais.
tensorflow==2.16.1: Uma biblioteca de aprendizado profundo para construir e treinar modelos de redes neurais.
shap==0.45.0: Uma biblioteca para explicar modelos de aprendizado de máquina, ajudando a entender como eles chegam a suas previsões.
flask==3.0.3: Um framework web para construir aplicativos web em Python.
python-Levenshtein==0.25.1: Uma biblioteca para calcular a distância de Levenshtein entre duas strings.
streamlit==1.32.1: Uma biblioteca para criar aplicativos web interativos para ciência de dados e aprendizado de máquina.
matplotlib==3.8.3: Uma biblioteca para criar gráficos e visualizações de dados em Python.
seaborn==0.13.2: Uma biblioteca de visualização de dados baseada no Matplotlib, com uma interface mais alta e gráficos mais bonitos.


----------
Conteúdo arquivo: config.yaml
Mascarar usuário e senha para acesso ao banco de dados no projeto. Não envie esse arquivo para o servidor, inclua no arquivo .gitignore
Nota: O arquivo deve contar dentro do repositório de exploratória.
----------
database_config:
  dbname: 'novadrivebank'
  user: 'etlreadonlybank'
  password: 'novadrive376A@'
  host: '159.223.187.110'
----------
//////////
----------
Conteúdo arquivo: const.py
Nota: O arquivo deve contar dentro do repositório de exploratória.
----------
consulta_sql = '''
SELECT c.Profissao,
       c.TempoProfissao,
       c.Renda,
       c.TipoResidencia,
       c.Escolaridade,
       c.Score,
       EXTRACT(YEAR FROM AGE(c.DataNascimento)) AS Idade,
       c.Dependentes,
       c.EstadoCivil,
       pf.NomeComercial AS Produto,
       pc.ValorSolicitado,
       pc.ValorTotalBem,
       CASE 
           WHEN COUNT(p.Status) FILTER (WHERE p.Status = 'Vencido') > 0 THEN 'ruim'
           ELSE 'bom'
       END AS Classe
FROM clientes c
JOIN PedidoCredito pc ON c.ClienteID = pc.ClienteID
JOIN ProdutosFinanciados pf ON pc.ProdutoID = pf.ProdutoID
LEFT JOIN ParcelasCredito p ON pc.SolicitacaoID = p.SolicitacaoID
WHERE pc.Status = 'Aprovado'
GROUP BY c.ClienteID, pf.NomeComercial, pc.ValorSolicitado, pc.ValorTotalBem
'''
----------
//////////
----------
Conteúdo arquivo: utils.py
Nota: O arquivo deve contar dentro do repositório de exploratória.
----------
import pandas as pd
import yaml 
import psycopg2 # biblioteca para conexão com o postgresql

def fetch_data_from_db(sql_query):
    try:
        with open('config.yaml', 'r') as file:
            config = yaml.safe_load(file)

        con = psycopg2.connect(
            dbname=config['database_config']['dbname'], 
            user=config['database_config']['user'], 
            password=config['database_config']['password'], 
            host=config['database_config']['host']
        )

        cursor = con.cursor()
        cursor.execute(sql_query)

        df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])

    finally:
        if 'cursor' in locals():
            cursor.close()
        if 'con' in locals():
            con.close()

    return df
----------
//////////
----------              

Observação: O código será exclusivo para Análise Exploratória 
- Não será usado para Limpeza/ML/Deploy

3.2 - EDA
3.2.1   - Busca obter informações sobre: 
3.2.1.1 - Variações
3.2.1.2 - Anomalias
3.2.1.3 - Distribuição dos dados
3.2.1.4 - Domínio

3.2.2   - O que buscamos:
3.2.2.1 - Distribuição / Outliers (dados discrepantes)
3.2.2.2 - Nas (dados faltantes) / Nulos
3.2.2.3 - Fora do domínio

3.2.3 - Atributos Categóricos
3.2.3.1 - Gráficos de Barras. Mostra a frequência por categoria. Indica Valores fora do domínio.
3.2.3.2 - Valores nulos

3.2.4 - Atributos Numéricos
3.2.4.1 - Distribuição: Boxplot, Histograma, Resumo Estatístico
3.2.4.2 - Valores nulos

Gráfico Histograma - Gráfico de colunas
Gráfico Boxplot - Possui informações de: Outlier, Máximo, Q3, Mediana, Q1, Mínimo

Q1 (Primeiro Quartil): Corresponde ao valor abaixo do qual se encontram 25% dos dados. É o limite inferior da caixa no boxplot.
Q3 (Terceiro Quartil): Corresponde ao valor abaixo do qual se encontram 75% dos dados. É o limite superior da caixa no boxplot.

----------
//////////
----------              
Conteúdo arquivo: exploratoria.py
!!!EDA - ANALISE EXPLORATÓRIA DE DADOS sendo realizada!!!
Nota: O arquivo deve contar dentro do repositório de exploratória.
----------

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import const
from utils import *

# Importa as bibliotecas necessárias para manipulação de dados (pandas), visualização (matplotlib, seaborn) 
# e módulos personalizados (const, utils).

df = fetch_data_from_db(const.consulta_sql)

# Carrega os dados de um banco de dados utilizando a função fetch_data_from_db e a consulta SQL definida em const.consulta_sql.
# O resultado é armazenado em um DataFrame pandas chamado df.

df['idade'] = df['idade'].astype(int)
df['valorsolicitado'] = df['valorsolicitado'].astype(float)
df['valortotalbem'] = df['valortotalbem'].astype(float)

# Converte as colunas 'idade', 'valorsolicitado' e 'valortotalbem' para os tipos de dados int e float, respectivamente.
# Isso é importante para realizar cálculos e análises numéricas.

variaveis_categoricas = ['profissao', 'tiporesidencia', 
                         'escolaridade', 'score', 'estadocivil', 'produto']
variaveis_numericas = ['tempoprofissao', 'renda', 'idade',
                        'dependentes', 'valorsolicitado', 'valortotalbem']

# Define listas com os nomes das colunas categoricas e numéricas do DataFrame.
# Essas listas serão utilizadas para iterar sobre as colunas e gerar os gráficos.

for coluna in variaveis_categoricas:
    df[coluna].value_counts().plot(kind='bar', figsize=(10, 6))
    plt.title(f'Distribuição de {coluna}')
    plt.ylabel('Contagem')
    plt.xlabel(coluna)
    plt.xticks(rotation=45)
    plt.show()

# Itera sobre cada coluna categórica e:
# - Conta a frequência de cada categoria usando value_counts().
# - Cria um gráfico de barras para visualizar a distribuição das categorias.
# - Personaliza o gráfico com título, rótulos dos eixos e rotação dos rótulos do eixo x.

for coluna in variaveis_numericas:
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=df, x=coluna)
    plt.title(f'Boxplot de {coluna}')
    plt.show()

    df[coluna].hist(bins=20, figsize=(10, 6))
    plt.title(f'Histograma de {coluna}')
    plt.xlabel(coluna)
    plt.ylabel('Frequência')
    plt.show()

    print(f'Resumo estatístico de :\n', df[coluna].describe(), '\n')

# Itera sobre cada coluna numérica e:
# - Cria um boxplot para visualizar a distribuição, quartis e outliers.
# - Cria um histograma para visualizar a frequência de cada intervalo de valores.
# - Imprime um resumo estatístico da coluna, incluindo média, desvio padrão, quartis, etc.

nulos_por_coluna = df.isnull().sum()
print(nulos_por_coluna)

# Conta a quantidade de valores nulos em cada coluna do DataFrame e imprime o resultado.
# Isso é útil para identificar colunas com muitos valores faltantes que podem precisar de tratamento.

----------
//////////
---------- 
3.2.5 - Conclusões EDA
3.2.5.1 - Dados inválidos: Profissão, Tempo de Profissão e Idade
3.2.5.2 - Dados Nulos: Profissão, Tipo de Residência

4 - Pré-processamento
- Converter Tipos
- Tratamento de Nulos
- Erros de digitação
- Tratamento de Outliers
- Feature Engineering
- Divisão em treino e Teste
- Normalização
- Codificação
- Seleção de atributos

4.1 - Tratamento de nulos
- Algoritmos de ML, especialmente RNA (Rede Neurais Artificiais), não conseguem lidar com valores nulos.
- O que fazer: 
--Remover a linha
--Substituir a ocorrência, mantendo a linha
---Não buscamos a contribuição individual de cada evento




----------
Código - utils.py

def substitui_nulos(df):
    """
    Preenche valores ausentes (NaN) no DataFrame.

    Para colunas do tipo string (object), utiliza a moda (valor mais frequente).
    Para colunas numéricas, utiliza a mediana.

    Args:
        df (pd.DataFrame): DataFrame com valores ausentes.

    Returns:
        pd.DataFrame: DataFrame com valores ausentes preenchidos.
    """
    for coluna in df.columns:
        if df[coluna].dtype == 'object':
            # Preenche com a moda (valor mais frequente) para colunas de texto
            moda = df[coluna].mode()[0]
            df[coluna].fillna(moda, inplace=True)
        else:
            # Preenche com a mediana para colunas numéricas
            mediana = df[coluna].median()
            df[coluna].fillna(mediana, inplace=True)
----------

Como?
- Dados categóricos: Moda
- Dados numéricos: Mediana

Nota: 
- Os arquivos devem constar na pasta limpeza: .gitignore, config.yaml, const.py, requirements.txt e utils.py
- O arquivo: modelcreation.py, será rodado no VSCode, mas também poderá ser rodado no Jupyter Notebook

4.2 - Erros de Entrada  - Dados Categóricos
- Vamos usar o domínio da categoria: valores possíveis
- Não é possível fazer função genérica e universal
Observação: Valores fora do domínio podem gerar erro no modelo

----------
Código - utils.py

def corrigir_erros_digitacao(df, coluna, lista_valida):
    """
    Corrige erros de digitação em uma coluna específica, utilizando o pacote fuzzywuzzy.

    Para cada valor na coluna, verifica se ele existe na lista_valida.
    Se não existir, utiliza o algoritmo de similaridade do fuzzywuzzy para sugerir a correção mais provável.

    Args:
        df (pd.DataFrame): DataFrame contendo a coluna com erros de digitação.
        coluna (str): Nome da coluna a ser corrigida.
        lista_valida (list): Lista de valores válidos para a coluna.

    Returns:
        None: Modifica o DataFrame original por referência.
    """
    for i, valor in enumerate(df[coluna]):
        valor_str = str(valor) if pd.notnull(valor) else valor  # Converte para string (se não for nulo)

        if valor_str not in lista_valida and pd.notnull(valor_str):
            # Sugere correção usando o algoritmo de similaridade do fuzzywuzzy
            correcao = process.extractOne(valor_str, lista_valida)[0]
            df.at[i, coluna] = correcao

----------

4.3 - Outliers
- Abordagem estatística: Desvio padrão ou Intervalo Interquartil
- Intervalo Interquartil: (IIQ) é uma medida estatística que indica a dispersão dos dados em um conjunto. 
Em outras palavras, ele nos mostra o quão espalhados os valores estão em torno da mediana.

Como ele funciona?
Quartis: Para calcular o IIQ, primeiro dividimos os dados em quatro partes iguais. Os pontos que dividem esses quartos são chamados de quartis.
Primeiro quartil (Q1): Separa os 25% menores valores do conjunto.
Terceiro quartil (Q3): Separa os 25% maiores valores do conjunto.
Cálculo do IIQ: O IIQ é a diferença entre o terceiro e o primeiro quartil:

- Abordagem de negócio: Domínio do atributo
--Tempo de profissão: 0-70
--Idade: 0-110

----------
Código - utils.py

def tratar_outliers(df, coluna, minimo, maximo):
    # Calcula a mediana dos valores da coluna, excluindo outliers
    mediana = df[(df[coluna] >= minimo) & (df[coluna] <= maximo)][coluna].median()
    # Substitui os outliers (valores abaixo de 'minimo' ou acima de 'maximo') pela mediana calculada
    df[coluna] = df[coluna].apply(lambda x: mediana if x < minimo or x > maximo else x)
    # Retorna o DataFrame com os outliers tratados
    return df

----------
Arquivo VSCode: UTILS.PY 
----------
# Importa bibliotecas necessárias
from fuzzywuzzy import process  # Para correção aproximada de erros de digitação
import pandas as pd             # Para manipulação de dados em dataframes
from sklearn.preprocessing import StandardScaler, LabelEncoder  # Para escalonamento e codificação de variáveis
import joblib                   # Para salvar e carregar modelos
import yaml                     # Para leitura de arquivos de configuração YAML
import psycopg2                 # Para conexão com banco de dados Postgres

# Importa constantes (provavelmente definidas em um arquivo separado)
import const                  

# Função para buscar dados do banco de dados
def fetch_data_from_db(sql_query):
    """ 
    Busca dados do banco de dados baseado em uma consulta SQL.

    Args:
        sql_query (str): Consulta SQL a ser executada.

    Returns:
        pd.DataFrame: DataFrame contendo os dados obtidos da consulta.
    """
    try:
        # Lê as configurações do banco de dados do arquivo config.yaml
        with open('config.yaml', 'r') as file:
            config = yaml.safe_load(file)

        # Estabelece conexão com o banco de dados usando as credenciais do arquivo de configuração
        con = psycopg2.connect(
            dbname=config['database_config']['dbname'],
            user=config['database_config']['user'],
            password=config['database_config']['password'],
            host=config['database_config']['host']
        )

        # Cria um cursor para executar a consulta
        cursor = con.cursor()
        cursor.execute(sql_query)

        # Lê os resultados da consulta e converte em um DataFrame do Pandas
        df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])

    finally:
        # Fecha o cursor e a conexão com o banco de dados (se foram criados)
        if 'cursor' in locals():
            cursor.close()
        if 'con' in locals():
            con.close()

    return df

def substitui_nulos(df):
    """
    Preenche valores ausentes (NaN) no DataFrame.

    Para colunas do tipo string (object), utiliza a moda (valor mais frequente).
    Para colunas numéricas, utiliza a mediana.

    Args:
        df (pd.DataFrame): DataFrame com valores ausentes.

    Returns:
        pd.DataFrame: DataFrame com valores ausentes preenchidos.
    """
    for coluna in df.columns:
        if df[coluna].dtype == 'object':
            # Preenche com a moda (valor mais frequente) para colunas de texto
            moda = df[coluna].mode()[0]
            df[coluna].fillna(moda, inplace=True)
        else:
            # Preenche com a mediana para colunas numéricas
            mediana = df[coluna].median()
            df[coluna].fillna(mediana, inplace=True)

def corrigir_erros_digitacao(df, coluna, lista_valida):
    """
    Corrige erros de digitação em uma coluna específica, utilizando o pacote fuzzywuzzy.

    Para cada valor na coluna, verifica se ele existe na lista_valida.
    Se não existir, utiliza o algoritmo de similaridade do fuzzywuzzy para sugerir a correção mais provável.

    Args:
        df (pd.DataFrame): DataFrame contendo a coluna com erros de digitação.
        coluna (str): Nome da coluna a ser corrigida.
        lista_valida (list): Lista de valores válidos para a coluna.

    Returns:
        None: Modifica o DataFrame original por referência.
    """
    for i, valor in enumerate(df[coluna]):
        valor_str = str(valor) if pd.notnull(valor) else valor  # Converte para string (se não for nulo)

        if valor_str not in lista_valida and pd.notnull(valor_str):
            # Sugere correção usando o algoritmo de similaridade do fuzzywuzzy
            correcao = process.extractOne(valor_str, lista_valida)[0]
            df.at[i, coluna] = correcao

def tratar_outliers(df, coluna, minimo, maximo):
    # Calcula a mediana dos valores da coluna, excluindo outliers
    mediana = df[(df[coluna] >= minimo) & (df[coluna] <= maximo)][coluna].median()
    # Substitui os outliers (valores abaixo de 'minimo' ou acima de 'maximo') pela mediana calculada
    df[coluna] = df[coluna].apply(lambda x: mediana if x < minimo or x > maximo else x)
    # Retorna o DataFrame com os outliers tratados
    return df

def save_scalers(df, nome_colunas):
	# salvar escalonadores em um arquivo, para posterior reutilização. 
    # Itera sobre as colunas especificadas
    for nome_coluna in nome_colunas:
        # Cria um objeto StandardScaler
        scaler = StandardScaler()
        # Aplica o scaler à coluna, ajustando e transformando os dados
        df[nome_coluna] = scaler.fit_transform(df[[nome_coluna]])
        # Salva o scaler em um arquivo joblib para uso futuro
        joblib.dump(scaler, f"./objects/scaler{nome_coluna}.joblib")
    # Retorna o DataFrame com as colunas escalonadas
    return df

def save_encoders(df, nome_colunas):
	# salvar parametros de codificadores em um arquivo, para posterior reutilização
    # Itera sobre as colunas especificadas
    for nome_coluna in nome_colunas:
        # Cria um objeto LabelEncoder
        label_encoder = LabelEncoder()
        # Aplica o encoder à coluna, convertendo valores categóricos em numéricos
        df[nome_coluna] = label_encoder.fit_transform(df[nome_coluna])
        # Salva o encoder em um arquivo joblib para uso futuro
        joblib.dump(label_encoder, f"./objects/labelencoder{nome_coluna}.joblib")
    # Retorna o DataFrame com as colunas codificadas
    return df

----------

4.4 - Engenharia de Atributos
- Codificação de Variáveis Categóricas
- Normalização e Padronização
- Tratamento de Valores Ausentes
- Interactions
df['proporcaosolicitadototal'] = df['valorsolicitado'] / df['valortotalbem']  # Cria uma nova coluna com a proporção entre valor solicitado e valor total do bem



4.5 - Divisão de Dados em Treino e Teste

			  ======TREINAR======RNA=======|
			  |		X_Treino			   |
			  |		y_treino			   |
  DADOS=======|							   |====MODELO=====PREVISÃO=====PERFORMANCE
 X_treino	  |                            |							  Previsão
 X_teste      | 						   |							   y_teste
 y_treino     ======TESTAR=====X_teste=====|
 y_teste			X_teste
					y_teste
					
* RNA - Redes Neurais Artificiais

----------
Código: modelcreation.py

#Divisão dos dados em treino e teste:
X = df.drop('classe', axis=1)  # Separando as features (X) da classe (y)
y = df['classe']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)  # Dividindo os dados em conjuntos de treino e teste

----------
4.6 - Normalização de DADOS
- Processo de transformação de dados numéricos
- Variáveis em escalas diferentes
--Contribuem de forma desbalanceada para o modelo 
--Exemplo: Salário e Altura
----------
Código: modelcreation.py

#Normalização:
X_test = save_scalers(X_test, ['tempoprofissao', ...])  # Normaliza as colunas numéricas do conjunto de teste
X_train = save_scalers(X_train, ['tempoprofissao', ...])  # Normaliza as colunas numéricas do conjunto de treino

----------

4.7 - Padronização (Z-Score)
- Dados aproximados da média (zero) e desvio padrão 1
- Podem ser negativos
- Não afeta outliers 
- Deve ser usado na maioria dos casos

4.8 - Normalização (Min-Max)
- Transforma para escala comum entre zero e 1 
- Usado em processamento de imagens e RNA
- Quando não sabemos a distribuição dos dados
- Quando precisam ser positivos
- Algoritmos não "requerem" dados normais
- Remove outliers pois impõem "limites"

Função – Normalização de Dados

----------

Código: utils.py

def save_scalers(df, nome_colunas):
	# salvar escalonadores em um arquivo, para posterior reutilização. 
    # Itera sobre as colunas especificadas
    for nome_coluna in nome_colunas:
        # Cria um objeto StandardScaler
        scaler = StandardScaler()
        # Aplica o scaler à coluna, ajustando e transformando os dados
        df[nome_coluna] = scaler.fit_transform(df[[nome_coluna]])
        # Salva o scaler em um arquivo joblib para uso futuro
        joblib.dump(scaler, f"./objects/scaler{nome_coluna}.joblib")
    # Retorna o DataFrame com as colunas escalonadas
    return df

----------

Porque salvar objetos?
- Uso do modelo treinado em produção

4.9 - Codificação Categórica ou Categorical Encoding
- Algoritmos entendem números
- Categorical Encoding é o processo de transformar categorias em números
-Duas formas:
--Label encoding
--One-hot encoding


4.9.1 - Label encoding 
- Cada categoria recebe um número, normalmente em ordem alfabética
--Ex: EstadoCivil: Casado, Divorciado, Solteiro
--Ex: EstadoCivil: Casado = 0, Divorciado = 1, Solteiro = 2
--Passa-se a considerar os números e não mais as nomenclaturas

----------
Código: utils.py

def save_encoders(df, nome_colunas):
	# salvar parametros de codificadores em um arquivo, para posterior reutilização
    # Itera sobre as colunas especificadas
    for nome_coluna in nome_colunas:
        # Cria um objeto LabelEncoder
        label_encoder = LabelEncoder()
        # Aplica o encoder à coluna, convertendo valores categóricos em numéricos
        df[nome_coluna] = label_encoder.fit_transform(df[nome_coluna])
        # Salva o encoder em um arquivo joblib para uso futuro
        joblib.dump(label_encoder, f"./objects/labelencoder{nome_coluna}.joblib")
    # Retorna o DataFrame com as colunas codificadas
    return df

----------

4.9.2 - Seleção de Atributos
- Um subconjunto de atributos pode criar um modelo com performance melhor 

----------

Código: modelcreation.py

# Seleção de Atributos
model = RandomForestClassifier()
# Instancia o RFE
selector = RFE(model, n_features_to_select=10, step=1)
selector = selector.fit(X_train, y_train)
# Transforma os dados
X_train = selector.transform(X_train)
X_test = selector.transform(X_test)
joblib.dump(selector, './objects/selector.joblib')

----------

Observações:

- Funções customizadas:  substitui_nulos, corrigir_erros_digitacao, tratar_outliers, save_scalers e save_encoders provavelmente estão definidas no módulo utils
e realizam tarefas específicas de pré-processamento de dados.
- Random Forest: A seleção de atributos é feita usando um modelo de Random Forest, que é um algoritmo de aprendizado de máquina popular para classificação.
- RFE: O RFE (Recursive Feature Elimination) é uma técnica para selecionar as features mais importantes em um conjunto de dados.
- Joblib: O Joblib é usado para salvar o objeto RFE, permitindo que ele seja carregado e reutilizado em outros scripts.

----------
Arquivo VSCode: modelcreation.py 
Arquivo Jupyter Notebook: modelcreation.ipynb 
----------

#Importando bibliotecas:
import pandas as pd  # Para manipulação de dados em dataframes
from datetime import datetime  # Para trabalhar com datas e horários
import numpy as np  # Para operações numéricas em arrays
import random as python_random  # Para gerar números aleatórios
import joblib  # Para salvar e carregar modelos e outros objetos Python

#Importando módulos de machine learning:
from sklearn.preprocessing import StandardScaler, LabelEncoder  # Para normalização e codificação de dados
from sklearn.model_selection import train_test_split  # Para dividir os dados em conjuntos de treino e teste
from sklearn.metrics import classification_report, confusion_matrix  # Para avaliar o modelo
from sklearn.ensemble import RandomForestClassifier  # Algoritmo de classificação
from sklearn.feature_selection import RFE  # Para seleção de atributos
import tensorflow as tf  # Biblioteca para deep learning (não utilizado neste código)

#Importando módulos customizados:
from utils import *  # Importa funções auxiliares definidas no módulo utils
import const  # Importa constantes definidas no módulo const

#Definindo a semente para reprodutibilidade:
seed = 41  # Valor da semente para gerar números aleatórios
np.random.seed(seed)  # Define a semente para o gerador de números aleatórios do NumPy
python_random.seed(seed)  # Define a semente para o gerador de números aleatórios padrão do Python
tf.random.set_seed(seed)  # Define a semente para o gerador de números aleatórios do TensorFlow
  
#Carregando os dados:
df = fetch_data_from_db(const.consulta_sql)  # Carrega os dados de um banco de dados usando a consulta SQL definida em const.consulta_sql

#Conversão de tipos:
df['idade'] = df['idade'].astype(int)  # Converte a coluna 'idade' para o tipo inteiro
df['valorsolicitado'] = df['valorsolicitado'].astype(float)  # Converte as colunas 'valorsolicitado' e 'valortotalbem' para o tipo float
df['valortotalbem'] = df['valortotalbem'].astype(float)

#Tratamento de dados faltantes:
substitui_nulos(df)  # Chama uma função (definida em utils) para substituir valores nulos nas colunas

#Tratamento de erros de digitação:
profissoes_validas = ['Advogado', 'Arquiteto', 'Cientista de Dados', 'Contador','Dentista','Empresário','Engenheiro','Médico','Programador']  # Lista de profissões válidas
corrigir_erros_digitacao(df, 'profissao', profissoes_validas)  # Corrige erros de digitação na coluna 'profissao'

#Tratamento de outliers:
df = tratar_outliers(df, 'tempoprofissao', 0, 70)  # Remove outliers da coluna 'tempoprofissao'
df = tratar_outliers(df, 'idade', 0, 110)  # Remove outliers da coluna 'idade'

#Engenharia de features:
df['proporcaosolicitadototal'] = df['valorsolicitado'] / df['valortotalbem']  # Cria uma nova coluna com a proporção entre valor solicitado e valor total do bem
df['proporcaosolicitadototal'] = df['proporcaosolicitadototal'].astype(float) # Transforma essa coluna em (float)

#Divisão dos dados em treino e teste:
X = df.drop('classe', axis=1)  # Separando as features (X) da classe (y)
y = df['classe']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)  # Dividindo os dados em conjuntos de treino e teste

#Normalização:
X_test = save_scalers(X_test, ['tempoprofissao', ...])  # Normaliza as colunas numéricas do conjunto de teste
X_train = save_scalers(X_train, ['tempoprofissao', ...])  # Normaliza as colunas numéricas do conjunto de treino

#Codificação:
mapeamento = {'ruim': 0, 'bom': 1}  # Mapa para converter valores textuais da classe em valores numéricos
y_train = np.array([mapeamento[item] for item in y_train])  # Codifica a classe no conjunto de treino
y_test = np.array([mapeamento[item] for item in y_test])  # Codifica a classe no conjunto de teste
X_train = save_encoders(X_train, ['profissao', ...])  # Codifica as colunas categóricas do conjunto de treino
X_test = save_encoders(X_test, ['profissao', ...])  # Codifica as colunas categóricas do conjunto de teste

#Seleção de atributos:
model = RandomForestClassifier()  # Cria um modelo de Random Forest
selector = RFE(model, n_features_to_select=10, step=1)  # Cria um objeto RFE para selecionar 10 atributos
selector = selector.fit(X_train, y_train)  # Ajusta o RFE aos dados de treino
X_train = selector.transform(X_train)  # Transforma os dados de treino usando o RFE
X_test = selector.transform(X_test)  # Transforma os dados de teste usando o RFE
joblib.dump(selector, './objects/selector.joblib')  # Salva o objeto RFE para uso futuro

----------

5 - Modelo

5.1 - Funcionamento: Rede Neural Artificial

5.2 - Funções de ativação: Relu e Sigmoid

5.3 - Relu (Rectified Linear Unit)
- Como funciona: A Relu é uma função muito simples: ela retorna o próprio valor de entrada se este for positivo, e zero caso contrário. 
Em outras palavras, ela "corta" a parte negativa do gráfico.
- Vantagens: 
-- Calculada rapidamente: A Relu é computacionalmente eficiente, o que acelera o treinamento da rede neural.
-- Evita o problema do gradiente vanishing: Ao contrário de outras funções, a derivada da Relu não se aproxima de zero para valores grandes, 
o que ajuda a rede a aprender mais rapidamente.
- Desvantagens:
-- Neurônios "mortos": Se um neurônio receber apenas entradas negativas durante o treinamento, ele sempre terá uma saída zero e não aprenderá mais.

5.4 - Sigmoid
- Como funciona: A função Sigmoid transforma qualquer valor de entrada em um número entre 0 e 1. Ela é frequentemente usada para modelar probabilidades.
- Vantagens:
-- Suave e diferenciável: A Sigmoid é uma função contínua e diferenciável em todos os pontos, o que é importante para o algoritmo de backpropagation (usado para ajustar os pesos da rede).
- Desvantagens:
-- Gradiente vanishing: A derivada da Sigmoid se aproxima de zero para valores muito grandes ou muito pequenos, o que pode desacelerar o treinamento da rede.
-- Saída limitada: A saída da Sigmoid está sempre entre 0 e 1, o que pode limitar a capacidade da rede de modelar dados complexos.

Quando usar cada uma?

Relu: É a função de ativação mais popular atualmente, especialmente nas camadas ocultas de uma rede neural. 
Ela é uma boa escolha para a maioria dos problemas, mas pode não ser ideal quando a rede tem muitos neurônios "mortos".

Sigmoid: É mais frequentemente usada na camada de saída quando queremos modelar probabilidades (por exemplo, em problemas de classificação binária).

Outras funções de ativação:
Além da Relu e da Sigmoid, existem outras funções de ativação, como a Tanh, Leaky Relu, ELU, etc. 
A escolha da função de ativação depende do problema específico e da arquitetura da rede neural.

Em resumo:

Relu: Rápida, eficiente e popular, mas pode ter problemas com neurônios "mortos".
Sigmoid: Suave e diferenciável, mas pode sofrer com o problema do gradiente vanishing.

5.5 - Função de Perda
- Mede a diferença entre a previsão (ou saida da rede) e o valor real.
- Binary Crossentropy: usada em classificação binária.

5.6 - Otimizador
- Busca minimizar a função de perda
- Adam (Adaptive Moment Estimation) é um dos estimadores mais comuns
- Taxa de Aprendizado: Valor com o qual os pesos serão ajustados. 
-- Definido valor inicial, otimizador ajusta o mesmo.

5.7 - Conceitos Gerais
- Epochs
- Batch-size

5.8 - Dropout
- Sinapses são removidas de forma aleatória
- Ajuda a minimizar overfitting e a convergência da rede



5.9 - Matriz de Confusão (Classificação)

------------
			     Classe                          Classe                   
			     Real                            Real
                 Positiva                        Negativa 

Classe       	Verdadeiros                      Falsos 
Prevista 		Positivos	                     Negativos
Positiva

Classe			Falsos 		                     Verdadeiros
Prevista		Positivos	                     Negativos
Negativa

----------

				Prev. Good        Prev.Bad
Verd. Good		    4				  1     
	
Verd. Bad			2				  3

----------
				
5.9.1 - Acurácia
- Classificação corretas
-- Formula: (VP + VN) / Total
-- Calculation: (4 + 3) / (4 + 1 + 2 + 3) = 7 / 10 = 0.7
-- Interpretação: O modelo classificou corretamente 70% das instancias


5.9.2 - Precisão
- A proporção de instancias verdadeiramente positivas entre as instancias previstas como positivas.
Fórmula: VP / (VP + FP)
Cálculo: 4 / (4 + 2) = 4 / 6 = 0.67
Interpretação: Das instancias previstas como positivas, 67% eram realmente positivas

5.9.3 - Recall
- Recall (sensibilidade ou Taxa de Verdadeiros Positivos): A proporção de instancias classificadas como positivas em relação as instancias positivas reais.
Formula: VP / (VP + FN)
Calculo: 4 / (4 + 1) = 4 / 5 = 0,8
Interpretação: O modelo identificou corretamente 80% das instancias boas

5.9.4 - Técnicas para melhorar o modelo
- Early Stopping: interrompe o treinamento quando a perfomance começa a piorar
- Regularization: L1 e L2 penalizam pesos maiores e previnem overfitting
- Batch Normalization: Acelera o processo de treino estabilizando o processo de aprendizado
- Tunning de Hiper - Parâmetros: valores como epochs, camadas, neurônios

5.9.5 - Versão com mudanças
- Modelcreation_modified.py

Observação: Esse repositório necessita dos seguintes arquivos: config.yaml, const.py, modelcreation.py, modelcreation_modified.py, modelcreation.ipynb, requirements.txt, utils.py

----------
Código: config.yaml

database_config:
  dbname: 'novadrivebank'
  user: 'etlreadonlybank'
  password: 'novadrive376A@'
  host: '159.223.187.110'
 
----------

Código: modelcreation_modified.py

import pandas as pd  # Importa a biblioteca pandas para manipulação de dados.
from datetime import datetime  # Importa a biblioteca datetime para trabalhar com datas e horas.
import numpy as np  # Importa a biblioteca numpy para operações numéricas.
import random as python_random  # Importa a biblioteca random para geração de números aleatórios.
import joblib  # Importa a biblioteca joblib para salvar e carregar modelos.

from sklearn.preprocessing import StandardScaler, LabelEncoder  # Importa classes para normalização e codificação de dados.
from sklearn.model_selection import train_test_split  # Importa função para dividir dados em treinamento e teste.
from sklearn.metrics import classification_report, confusion_matrix  # Importa métricas de avaliação de modelos.
from sklearn.ensemble import RandomForestClassifier  # Importa o algoritmo de classificação Random Forest.
from sklearn.feature_selection import RFE  # Importa técnica de seleção de features Recursive Feature Elimination.
import tensorflow as tf  # Importa a biblioteca TensorFlow para construção de modelos de aprendizado profundo.
from tensorflow.keras.layers import BatchNormalization  # Importa camada de normalização de batch.
from tensorflow.keras.regularizers import l2  # Importa regularização L2.
from tensorflow.keras.layers import Dropout  # Importa camada de dropout.
from tensorflow.keras.callbacks import EarlyStopping  # Importa técnica de parada precoce.

from utils import *  # Importa funções auxiliares do módulo utils.

seed = 41  # Define uma semente para reprodutibilidade.
np.random.seed(seed)  # Configura a semente para o gerador de números aleatórios do numpy.
python_random.seed(seed)  # Configura a semente para o gerador de números aleatórios do random.
tf.random.set_seed(seed)  # Configura a semente para o gerador de números aleatórios do TensorFlow.

df = fetch_data_from_db(const.consulta_sql)  # Carrega dados de um banco de dados usando a consulta SQL definida em const.consulta_sql.

df['idade'] = df['idade'].astype(int)  # Converte a coluna 'idade' para tipo inteiro.
df['valorsolicitado'] = df['valorsolicitado'].astype(float)  # Converte a coluna 'valorsolicitado' para tipo float.
df['valortotalbem'] = df['valortotalbem'].astype(float)  # Converte a coluna 'valortotalbem' para tipo float.

# Define uma lista com as profissões válidas para o modelo
profissoes_validas = ['Advogado', 'Arquiteto', 'Cientista de Dados', ...]

# Corrige possíveis erros de digitação na coluna 'profissao' com base na lista de profissões válidas
corrigir_erros_digitacao(df, 'profissao', profissoes_validas)

# Trata outliers na coluna 'tempoprofissao', limitando os valores entre 0 e 70
df = tratar_outliers(df, 'tempoprofissao', 0, 70)
# Trata outliers na coluna 'idade', limitando os valores entre 0 e 110
df = tratar_outliers(df, 'idade', 0, 110)

# Cria uma nova coluna calculando a proporção entre o valor solicitado e o valor total do bem
df['proporcaosolicitadototal'] = df['valorsolicitado'] / df['valortotalbem']
# Converte a coluna para tipo float
df['proporcaosolicitadototal'] = df['proporcaosolicitadototal'].astype(float)

# Separa as features (X) da variável target (y)
X = df.drop('classe', axis=1)
y = df['classe']

# Divide os dados em conjuntos de treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)

# Aplica escalonamento aos dados numéricos de treino e teste
X_test = save_scalers(X_test, ['tempoprofissao','renda','idade',...])
X_train = save_scalers(X_train, ['tempoprofissao','renda','idade',...])

# Visualiza os primeiros registros do conjunto de teste
X_test.head()

# Mapeia os valores 'ruim' e 'bom' da variável target para 0 e 1, respectivamente
mapeamento = {'ruim': 0, 'bom': 1}
y_train = np.array([mapeamento[item] for item in y_train])
y_test = np.array([mapeamento[item] for item in y_test])

# Aplica codificação one-hot encoding às features categóricas de treino e teste
X_train = save_encoders(X_train, ['profissao', 'tiporesidencia', ...])
X_test = save_encoders(X_test, ['profissao', 'tiporesidencia', ...])

# Importa a classe RandomForestClassifier do scikit-learn
model = RandomForestClassifier()

# Cria um objeto RFE (Recursive Feature Elimination)
selector = RFE(model, n_features_to_select=10, step=1)

# Treina o RFE para selecionar as 10 features mais importantes
selector = selector.fit(X_train, y_train)

# Aplica a seleção de features em X_train
X_train = selector.transform(X_train)

# Aplica a seleção de features em X_test
X_test = selector.transform(X_test)

# Salva o objeto RFE para uso posterior (opcional)
joblib.dump(selector, 'selector.joblib')

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=l2(0.01), input_shape=(X_train.shape[1],)),
    # Cria a primeira camada densa da rede neural com 128 neurônios.
    # A função de ativação 'relu' (Rectified Linear Unit) introduz não-linearidade.
    # O regularizador L2 com peso 0.01 é aplicado para evitar overfitting.
    # O argumento 'input_shape' especifica a forma dos dados de entrada (número de features).

    tf.keras.layers.Dropout(0.3),
    # Aplica dropout com taxa de 0.3, desativando aleatoriamente 30% dos neurônios durante o treinamento
    # para prevenir overfitting.

    tf.keras.layers.Dense(64, kernel_regularizer=l2(0.01), activation='relu'),
    # Cria a segunda camada densa com 64 neurônios, usando relu e L2 regularização.

    tf.keras.layers.Dropout(0.3),
    # Aplica dropout com taxa de 0.3.

    tf.keras.layers.Dense(32, kernel_regularizer=l2(0.01), activation='relu'),
    # Cria a terceira camada densa com 32 neurônios, usando relu e L2 regularização.

    tf.keras.layers.Dropout(0.3),
    # Aplica dropout com taxa de 0.3.

    tf.keras.layers.Dense(1, kernel_regularizer=l2(0.01), activation='sigmoid')
    # Cria a camada de saída com 1 neurônio (para problemas de classificação binária)
    # e função de ativação sigmoid para obter probabilidades entre 0 e 1.
    # A regularização L2 também é aplicada nesta camada.
])

# Importa a classe Adam do módulo de otimizadores do Keras
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

# Compila o modelo com o otimizador Adam, função de perda de entropia cruzada binária
# e métrica de acurácia
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Adiciona uma camada de dropout com taxa de dropout de 50% para regularização
model.add(Dropout(0.5))

# Treina o modelo
model.fit(
    # Dados de treinamento
    X_train,
    y_train,
    # Callback para parar o treinamento precocemente se a perda de validação não melhorar por 10 épocas
    callbacks=[EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)],
    # Separa 20% dos dados de treinamento para validação
    validation_split=0.2,
    # Número máximo de iterações sobre o conjunto de dados de treinamento
    epochs=500,
    # Tamanho do lote de dados utilizado em cada atualização dos pesos
    batch_size=10,
    # Nível de detalhe da saída durante o treinamento
    verbose=1
)

# Salva o modelo treinado em um arquivo chamado 'meu_modelo.keras'
model.save('meu_modelo.keras')

# Faz previsões utilizando o modelo treinado nos dados de teste (X_test)
y_pred = model.predict(X_test)

# Converte as probabilidades de previsão em classes (0 ou 1)
# Se a probabilidade for maior que 0.5, a classe é considerada 1, caso contrário, 0
y_pred = (y_pred > 0.5).astype(int) 

# Imprime uma mensagem indicando que a avaliação do modelo será feita nos dados de teste
print("Avaliação do Modelo nos Dados de Teste:")

# Avalia o modelo usando os dados de teste (X_test) e as verdadeiras classes (y_test)
# Retorna métricas de avaliação como perda e acurácia
model.evaluate(X_test, y_test)

# Imprime um cabeçalho indicando que será apresentado um relatório de classificação
print("\nRelatório de Classificação:")

# Gera um relatório detalhado de classificação, incluindo métricas como precisão, recall, F1-score, etc.
# Compara as classes previstas (y_pred) com as verdadeiras classes (y_test)
print(classification_report(y_test, y_pred))
----------
Código: modelcreation.py

# Importa a biblioteca pandas para manipulação de dados
import pandas as pd

# Importa a classe datetime da biblioteca datetime para trabalhar com datas e horas
from datetime import datetime

# Importa a biblioteca numpy para operações numéricas
import numpy as np

# Importa a biblioteca random e a renomeia como python_random para gerar números aleatórios
import random as python_random

# Importa a biblioteca joblib para salvar e carregar modelos
import joblib

# Importa classes e funções do scikit-learn para pré-processamento, divisão de dados, métricas e modelos
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE

# Importa a biblioteca TensorFlow para construção e treinamento de modelos de aprendizado profundo
import tensorflow as tf

# Importa todas as funções e classes do módulo utils
from utils import *

# Importa o módulo const
import const

# Define uma semente para garantir a reprodutibilidade dos resultados
seed = 41

# Define a semente para o gerador de números aleatórios do numpy
np.random.seed(seed)

# Define a semente para o gerador de números aleatórios do random
python_random.seed(seed)

# Define a semente para o gerador de números aleatórios do TensorFlow
tf.random.set_seed(seed)

# Obtém os dados brutos a partir do banco de dados utilizando uma consulta SQL definida em const.consulta_sql
df = fetch_data_from_db(const.consulta_sql)

# Converte a coluna 'idade' para o tipo inteiro
df['idade'] = df['idade'].astype(int)

# Converte a coluna 'valorsolicitado' para o tipo float
df['valorsolicitado'] = df['valorsolicitado'].astype(float)

# Converte a coluna 'valortotalbem' para o tipo float
df['valortotalbem'] = df['valortotalbem'].astype(float)

# Realiza o tratamento de valores nulos no DataFrame
substitui_nulos(df)

# Define uma lista de profissões válidas para correção de erros de digitação na coluna 'profissao'
profissoes_validas = ['Advogado', 'Arquiteto', 'Cientista de Dados', 'Contador', 'Dentista', 'Empresário',
                      'Engenheiro', 'Médico', 'Programador']

# Corrige erros de digitação na coluna 'profissao' utilizando a lista de profissões válidas
corrigir_erros_digitacao(df, 'profissao', profissoes_validas)

# Trata outliers na coluna 'tempoprofissao', limitando os valores entre 0 e 70
df = tratar_outliers(df, 'tempoprofissao', 0, 70)

# Trata outliers na coluna 'idade', limitando os valores entre 0 e 110
df = tratar_outliers(df, 'idade', 0, 110)

# Cria uma nova coluna 'proporcaosolicitadototal' calculando a proporção entre 'valorsolicitado' e 'valortotalbem'
df['proporcaosolicitadototal'] = df['valorsolicitado'] / df['valortotalbem']

# Converte a coluna 'proporcaosolicitadototal' para o tipo float
df['proporcaosolicitadototal'] = df['proporcaosolicitadototal'].astype(float)

# Separa os dados em variáveis independentes (X) e dependente (y)
X = df.drop('classe', axis=1)
y = df['classe']

# Divide os dados em conjuntos de treino e teste, utilizando 20% dos dados para teste e uma semente para reprodutibilidade
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)

# Normalização dos dados de teste utilizando a função save_scalers para as colunas especificadas
X_test = save_scalers(X_test, ['tempoprofissao', 'renda', 'idade',
                               'dependentes', 'valorsolicitado', 'valortotalbem', 'proporcaosolicitadototal'])

# Normalização dos dados de treino utilizando a função save_scalers para as colunas especificadas
X_train = save_scalers(X_train, ['tempoprofissao', 'renda', 'idade',
                                 'dependentes', 'valorsolicitado', 'valortotalbem', 'proporcaosolicitadototal'])

# Codificação das classes alvo (y_train e y_test) utilizando um mapeamento de valores categóricos para numéricos
mapeamento = {'ruim': 0, 'bom': 1}
y_train = np.array([mapeamento[item] for item in y_train])
y_test = np.array([mapeamento[item] for item in y_test])

# Codificação das variáveis categóricas nos dados de treino utilizando a função save_encoders
X_train = save_encoders(X_train, ['profissao', 'tiporesidencia', 
                                  'escolaridade', 'score', 'estadocivil', 'produto'])

# Codificação das variáveis categóricas nos dados de teste utilizando a função save_encoders
X_test = save_encoders(X_test, ['profissao', 'tiporesidencia', 
                                'escolaridade', 'score', 'estadocivil', 'produto'])

# Seleção de Atributos
# Instancia um modelo de RandomForestClassifier
model = RandomForestClassifier()

# Instancia o RFE (Recursive Feature Elimination) com o modelo e define o número de atributos a selecionar
selector = RFE(model, n_features_to_select=10, step=1)

# Ajusta o RFE aos dados de treino
selector = selector.fit(X_train, y_train)

# Transforma os dados de treino selecionando os atributos mais importantes
X_train = selector.transform(X_train)

# Transforma os dados de teste selecionando os atributos mais importantes
X_test = selector.transform(X_test)

# Salva o seletor RFE em um arquivo utilizando joblib
joblib.dump(selector, './objects/selector.joblib')

# Define a arquitetura do modelo de rede neural sequencial utilizando TensorFlow
model = tf.keras.Sequential([
    # Primeira camada densa com 128 neurônios e função de ativação ReLU
    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    # Camada de dropout com taxa de 0.3 para evitar overfitting
    tf.keras.layers.Dropout(0.3),
    # Segunda camada densa com 64 neurônios e função de ativação ReLU
    tf.keras.layers.Dense(64, activation='relu'),
    # Camada de dropout com taxa de 0.3 para evitar overfitting
    tf.keras.layers.Dropout(0.3),
    # Terceira camada densa com 32 neurônios e função de ativação ReLU
    tf.keras.layers.Dense(32, activation='relu'),
    # Camada de dropout com taxa de 0.3 para evitar overfitting
    tf.keras.layers.Dropout(0.3),
    # Camada de saída com 1 neurônio e função de ativação sigmoide para classificação binária
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Configurando o otimizador Adam com uma taxa de aprendizado de 0.001
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

# Compilando o modelo com o otimizador Adam, a função de perda 'binary_crossentropy' e a métrica 'accuracy'
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Treinamento do modelo
model.fit(
    X_train,  # Dados de treino
    y_train,  # Rótulos de treino
    validation_split=0.2,  # Usa 20% dos dados de treino para validação
    epochs=500,  # Número máximo de épocas para o treinamento
    batch_size=10,  # Tamanho do lote para cada iteração de treinamento
    verbose=1  # Exibe o progresso do treinamento
)

# Salvando o modelo treinado no formato .keras
model.save('meu_modelo.keras')

# Fazendo previsões nos dados de teste
y_pred = model.predict(X_test)

# Convertendo as previsões para valores binários (0 ou 1) com base em um limiar de 0.5
y_pred = (y_pred > 0.5).astype(int)

# Avaliando o modelo nos dados de teste
print("Avaliação do Modelo nos Dados de Teste:")
model.evaluate(X_test, y_test)

# Imprimindo o relatório de classificação com métricas de desempenho
print("\nRelatório de Classificação:")
print(classification_report(y_test, y_pred))

----------

Código: requirements.txt

fuzzywuzzy==0.18.0
pandas==2.2.1
scikit-learn==1.4.1.post1
joblib==1.3.2
PyYAML==6.0.1
psycopg2-binary==2.9.9
numpy==1.26.4
tensorflow==2.16.1
shap==0.45.0
flask==3.0.3
python-Levenshtein==0.25.1
streamlit==1.32.1
matplotlib==3.8.3
seaborn==0.13.2

----------

Código: utils.py

from fuzzywuzzy import process
import pandas as pd
from sklearn.preprocessing import StandardScaler,LabelEncoder
import joblib
import yaml
import psycopg2
import const

def fetch_data_from_db(sql_query):
    try:
        with open('config.yaml', 'r') as file:
            config = yaml.safe_load(file)

        con = psycopg2.connect(
            dbname=config['database_config']['dbname'], 
            user=config['database_config']['user'], 
            password=config['database_config']['password'], 
            host=config['database_config']['host']
        )

        cursor = con.cursor()
        cursor.execute(sql_query)

        df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])

    finally:
        if 'cursor' in locals():
            cursor.close()
        if 'con' in locals():
            con.close()

    return df

def substitui_nulos(df):
    for coluna in df.columns:
        if df[coluna].dtype == 'object':
            moda = df[coluna].mode()[0]
            df[coluna].fillna(moda, inplace=True)
        else:
            mediana = df[coluna].median()
            df[coluna].fillna(mediana, inplace=True)

def corrigir_erros_digitacao(df, coluna, lista_valida):
    for i, valor in enumerate(df[coluna]):
        valor_str = str(valor) if pd.notnull(valor) else valor

        if valor_str not in lista_valida and pd.notnull(valor_str):
            correcao = process.extractOne(valor_str, lista_valida)[0]
            df.at[i, coluna] = correcao

def tratar_outliers(df, coluna, minimo, maximo):
    mediana = df[(df[coluna] >= minimo) & (df[coluna] <= maximo)][coluna].median()
    df[coluna] = df[coluna].apply(lambda x: mediana if x < minimo or x > maximo else x)
    return df

def save_scalers(df, nome_colunas):
    for nome_coluna in nome_colunas:
        scaler = StandardScaler()
        df[nome_coluna] = scaler.fit_transform(df[[nome_coluna]])
        joblib.dump(scaler, f"./objects/scaler{nome_coluna}.joblib")

    return df

def save_encoders(df, nome_colunas):
    for nome_coluna in nome_colunas:
        label_encoder = LabelEncoder()
        df[nome_coluna] = label_encoder.fit_transform(df[nome_coluna])
        joblib.dump(label_encoder, f"./objects/labelencoder{nome_coluna}.joblib")

    return df
	
---------- 
            
6 - Explicabilidade

6.1 - O que é XAI?
- Significado de explicável: Adjetivo "Que se consegue explicar; em que há ou pode haver explicação"

6.2 - Relação I
- Complexidade
- Capacidade
- Explicabilidade

6.3 - Relação II
- Explicabilidade 
- Confiança

6.4 - Se você precisa de XAI
- Usar modelos com maior explicabilidade
- Usar técnicas que melhoram a explicabilidade dos modelos

6.5 - Aplicações
- Diagnostico médico
- Aprovação de Crédito
- Veiculos autônomos
- Decisões judiciais
- Contratação de funcionários

6.6 - Questões Legais
- Legislação obriga

6.7 - Reputação
- AI toma uma decisão preconceituosa

6.8 - Segurança Jurídica/Financeira
- Um veículo autônomo provoca um acidente
- Como planejar uma defesa?

6.9 - Depuração
- Normalmente o modelo é testado em termos de precisão e outras métricas, mas isso é suficiente?
- Enviesamento no treino

6.9.1 - Prevenção de ataques
- Um modelo pode ser hackeado

6.9.2 - Questão pessoal
- Executivo quer entender

6.9.3 - White Box / Black Box

- Black Box
-- Modelos não podem ser interpretados
--- Redes Neurais Artificiais
--- Modelos baseados em grupos
--- Maquina de Vetor de Suporte

- White Box
-- Modelo de fácil interpretação/entendimento
--- Regressão Linear
--- Arvores de Decisão
--- Modelos baseados em Regras

6.9.4 - Algoritmo vs Modelo
- Interpretar o algoritmo é uma visão Agnóstica de DADOS
- Interpretar o modelo é uma visão de caso especifico

6.9.5 - Interpretação Agnóstica vs Especifica
- Exemplo agnóstica: Lime
- Exemplo especifico: Tree Shap (arvores de decisão)

6.9.6 - Local interpretable Model-Agnostic Explanations (LIME)
- Meio que pesos são dados a cada item e mostrados (transparentes), de como conseguiu chegar na decisão.

6.9.7 - Visão mais detalhada
- Escolaridade <= 0.00: -0.195433
- 0.00 < tiporesidencia <=1.00: 0.0737412
- produto > 5.00: 0.044722

6.9.8 - Lime (Local Interpretable Model-Agnostic Explanations) 
- É uma técnica usada em IA explicável (XAI) para interpretar e explicar as previsões de modelos de aprendizado de máquina. Aqui está um resumo de como funciona:
-- Amostragem Local: LIME gera várias amostras perturbadas da instância que você deseja explicar. Essas amostras são ligeiramente modificadas em relação à instância original.
-- Predições do Modelo: O modelo complexo faz previsões para essas amostras perturbadas.
-- Modelo Simples: LIME ajusta um modelo simples (como uma regressão linear) às previsões do modelo complexo, mas apenas na vizinhança da instância original. 
----Esse modelo simples é mais fácil de interpretar.
-- Explicação: O modelo simples fornece uma explicação localmente fiel sobre como o modelo complexo está tomando decisões para aquela instância específica12.

LIME pode ser aplicado a diferentes tipos de dados, como tabulares, imagens ou texto, ajudando a validar o modelo e construir confiança em suas previsões.

Observação: Esse repositório necessita dos seguintes arquivos: config.yaml, const.py, modelcreation.py, modelcreation_modified.py, modelcreation.ipynb, requirements.txt, utils.py

----------
Código: xai.py

# Importa a biblioteca pandas para manipulação de dados
import pandas as pd

# Importa a classe datetime para trabalhar com datas e horas
from datetime import datetime

# Importa a biblioteca numpy para operações numéricas
import numpy as np

# Importa a biblioteca random com um alias python_random para gerar números aleatórios
import random as python_random

# Importa a biblioteca joblib para salvar e carregar modelos
import joblib

# Importa classes e funções do scikit-learn para pré-processamento, divisão de dados, métricas e modelos
from sklearn.preprocessing import StandardScaler, LabelEncoder  # Para normalização e codificação de rótulos
from sklearn.model_selection import train_test_split  # Para dividir os dados em conjuntos de treino e teste
from sklearn.metrics import classification_report, confusion_matrix  # Para avaliar o desempenho do modelo
from sklearn.ensemble import RandomForestClassifier  # Para criar um modelo de floresta aleatória
from sklearn.feature_selection import RFE  # Para seleção de características

# Importa a biblioteca TensorFlow para construção e treinamento de modelos de aprendizado profundo
import tensorflow as tf

# Importa funções e constantes de módulos personalizados
from utils import *
import const

# Define uma semente para garantir a reprodutibilidade dos resultados
seed = 41

# Define a semente para o gerador de números aleatórios do numpy
np.random.seed(seed)

# Define a semente para o gerador de números aleatórios do random
python_random.seed(seed)

# Define a semente para o gerador de números aleatórios do TensorFlow
tf.random.set_seed(seed)

# Obtém os dados brutos a partir do banco de dados utilizando uma consulta SQL definida em constantes
df = fetch_data_from_db(const.consulta_sql)

# Converte a coluna 'idade' para o tipo inteiro
df['idade'] = df['idade'].astype(int)
# Converte a coluna 'valorsolicitado' para o tipo float
df['valorsolicitado'] = df['valorsolicitado'].astype(float)

# Converte a coluna 'valortotalbem' para o tipo float
df['valortotalbem'] = df['valortotalbem'].astype(float)

# Chama a função para substituir valores nulos no DataFrame
substitui_nulos(df)

# Define uma lista de profissões válidas para correção de erros de digitação
profissoes_validas = ['Advogado', 'Arquiteto', 'Cientista de Dados', 'Contador', 'Dentista', 'Empresário', 'Engenheiro', 'Médico', 'Programador']

# Corrige erros de digitação na coluna 'profissao' utilizando a lista de profissões válidas
corrigir_erros_digitacao(df, 'profissao', profissoes_validas)

# Trata outliers na coluna 'tempoprofissao', limitando os valores entre 0 e 70
df = tratar_outliers(df, 'tempoprofissao', 0, 70)

# Trata outliers na coluna 'idade', limitando os valores entre 0 e 110
df = tratar_outliers(df, 'idade', 0, 110)

# Cria uma nova coluna 'proporcaosolicitadototal' calculando a proporção entre 'valorsolicitado' e 'valortotalbem'
df['proporcaosolicitadototal'] = df['valorsolicitado'] / df['valortotalbem']

# Converte a nova coluna 'proporcaosolicitadototal' para o tipo float
df['proporcaosolicitadototal'] = df['proporcaosolicitadototal'].astype(float)

# Separa os dados em variáveis independentes (X) e dependente (y)
X = df.drop('classe', axis=1)
y = df['classe']

# Divide os dados em conjuntos de treino e teste, utilizando 20% dos dados para teste e uma semente para reprodutibilidade
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)

# Normalização dos dados de teste
X_test = save_scalers(X_test, ['tempoprofissao', 'renda', 'idade', 'dependentes', 'valorsolicitado', 'valortotalbem', 'proporcaosolicitadototal'])

# Normalização dos dados de treino
X_train = save_scalers(X_train, ['tempoprofissao', 'renda', 'idade', 'dependentes', 'valorsolicitado', 'valortotalbem', 'proporcaosolicitadototal'])

# Mapeamento de rótulos de classe para valores numéricos
mapeamento = {'ruim': 0, 'bom': 1}

# Converte os rótulos de treino para valores numéricos usando o mapeamento
y_train = np.array([mapeamento[item] for item in y_train])

# Converte os rótulos de teste para valores numéricos usando o mapeamento
y_test = np.array([mapeamento[item] for item in y_test])

# Codificação das variáveis categóricas nos dados de treino
X_train = save_encoders(X_train, ['profissao', 'tiporesidencia', 'escolaridade', 'score', 'estadocivil', 'produto'])

# Codificação das variáveis categóricas nos dados de teste
X_test = save_encoders(X_test, ['profissao', 'tiporesidencia', 'escolaridade', 'score', 'estadocivil', 'produto'])

'''
# Seleção de Atributos usando RandomForestClassifier
model = RandomForestClassifier()

# Instancia o RFE (Recursive Feature Elimination) com o modelo e o número de características a selecionar
selector = RFE(model, n_features_to_select=10, step=1)

# Ajusta o RFE aos dados de treino
selector = selector.fit(X_train, y_train)

# Transforma os dados de treino selecionando as características mais importantes
X_train = selector.transform(X_train)

# Transforma os dados de teste selecionando as características mais importantes
X_test = selector.transform(X_test)

# Salva o seletor de características em um arquivo para uso futuro
joblib.dump(selector, './objects/selector.joblib')
'''

# Define a arquitetura do modelo de rede neural sequencial usando TensorFlow
model = tf.keras.Sequential([
    # Primeira camada densa com 128 neurônios e função de ativação ReLU
    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    
    # Camada de dropout para evitar overfitting, desativando 30% dos neurônios
    tf.keras.layers.Dropout(0.3),
    
    # Segunda camada densa com 64 neurônios e função de ativação ReLU
    tf.keras.layers.Dense(64, activation='relu'),
    
    # Camada de dropout para evitar overfitting, desativando 30% dos neurônios
    tf.keras.layers.Dropout(0.3),
    
    # Terceira camada densa com 32 neurônios e função de ativação ReLU
    tf.keras.layers.Dense(32, activation='relu'),
    
    # Camada de dropout para evitar overfitting, desativando 30% dos neurônios
    tf.keras.layers.Dropout(0.3),
    
    # Camada de saída com 1 neurônio e função de ativação sigmoid para classificação binária
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Configurando o otimizador Adam com uma taxa de aprendizado de 0.001
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

# Compilando o modelo com o otimizador Adam, função de perda binária (binary_crossentropy) e métrica de acurácia
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Treinamento do modelo
model.fit(
    X_train,  # Dados de treino
    y_train,  # Rótulos de treino
    validation_split=0.2,  # Usa 20% dos dados de treino para validação
    epochs=500,  # Número máximo de épocas para o treinamento
    batch_size=10,  # Tamanho do lote para cada iteração de treinamento
    verbose=1  # Exibe o progresso do treinamento
)

# Salvando o modelo treinado no formato Keras
model.save('meu_modelo.keras')

# Fazendo previsões nos dados de teste
y_pred = model.predict(X_test)

# Convertendo as previsões para valores binários (0 ou 1) com base em um limiar de 0.5
y_pred = (y_pred > 0.5).astype(int)

# Avaliando o modelo nos dados de teste
print("Avaliação do Modelo nos Dados de Teste:")
model.evaluate(X_test, y_test)

# Imprimindo o relatório de classificação com métricas de desempenho
print("\nRelatório de Classificação:")
print(classification_report(y_test, y_pred))

# Importa a biblioteca pandas para manipulação de dados em DataFrame
import pandas as pd
# Importa a biblioteca numpy para operações numéricas
import numpy as np
# Importa a biblioteca LIME para explicações de modelos
import lime
import lime.lime_tabular

# Função de Previsão Ajustada para LIME
def model_predict(data_asarray):
    # Converte o array de dados em um DataFrame, utilizando as colunas do conjunto de treino
    data_asframe = pd.DataFrame(data_asarray, columns=X_train.columns)
    
    # Aplica os scalers (normalizadores) nas colunas especificadas
    data_asframe = save_scalers(data_asframe, ['tempoprofissao', 'renda', 'idade', 'dependentes', 'valorsolicitado', 'valortotalbem', 'proporcaosolicitadototal'])
    
    # Aplica os encoders (codificadores) nas colunas categóricas especificadas
    data_asframe = save_encoders(data_asframe, ['profissao', 'tiporesidencia', 'escolaridade', 'score', 'estadocivil', 'produto'])
    
    # Realiza a previsão utilizando o modelo treinado
    predictions = model.predict(data_asframe)
    
    # Retorna as previsões ajustadas, combinando as probabilidades de cada classe
    return np.hstack((1-predictions, predictions))

# Cria o objeto explainer do LIME para explicar as previsões do modelo
explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, 
                feature_names=X_train.columns, class_names=['ruim', 'bom'], mode='classification')

# Explica a previsão para uma instância específica do conjunto de teste
exp = explainer.explain_instance(X_test.values[1], model_predict, num_features=10)

# Gera um arquivo HTML com a explicação da previsão
exp.save_to_file('lime_explanation.html')

# Imprime os recursos e seus pesos para a classe 'Bom'
print('\nImprimindo os recursos e seus pesos para Bom')
if 1 in exp.local_exp:
    for feature, weight in exp.local_exp[1]:
        print(f"{feature}: {weight}")

# Acessa os valores das features e seus pesos para a classe 'Bom'
print("\nAcessar os valores das features e seus pesos para Bom")
feature_importances = exp.as_list(label=1)
for feature, weight in feature_importances:
    print(f"{feature}: {weight}")
----------

7 - API
- Base de Dados
- Scalers
- Normalizadores
- Feature Selector
- Modelo
- Relatórios

7.1 - Como?
- API
- Flash

7.2 - O que vamos fazer?
- Criar aplicação Flask
- Criar um script python para testar o Flask
- Criar uma aplicação WEB Streamlit que vai consumir a API

7.3 -  Flask

7.4 - Streamlit
- Permite criar aplicações WEB sem código WEB
- Você roda a aplicação no arquivo py
- Pode publicar em qualquer lugar

7.5 - Roteiro
- Rodar modelcreation.py
- Criar aplicação Flask 
-- Url no config

- Criar script de teste - requisição para mais de uma previsão
--testflask.py

- Criar aplicação streamlit - requisição de uma previsão por vez
-- Testar com vários valores

7.6 - Comando para rodar webapp.py
python -m streamlit run .\webapp.py

Observação: Os arquivos devem constar no diretório API: api.py, const.py, modelcreation.py, modelcreation_modified.py, requirements.txt, testflask.py, utils.py, webapp.py, xai.py

----------
Código: config.yaml

database_config:
  dbname: 'novadrivebank'
  user: 'etlreadonlybank'
  password: 'novadrive376A@'
  host: '159.223.187.110'

url_api:
   url: 'http://127.0.0.1:5000/predict'
   #url: 'http://ec2-44-210-140-14.compute-1.amazonaws.com:5000/predict'

Observação: Aqui vai a configuração AWS.
----------

Código: const.py

consulta_sql = '''
SELECT c.Profissao,
       c.TempoProfissao,
       c.Renda,
       c.TipoResidencia,
       c.Escolaridade,
       c.Score,
       EXTRACT(YEAR FROM AGE(c.DataNascimento)) AS Idade,
       c.Dependentes,
       c.EstadoCivil,
       pf.NomeComercial AS Produto,
       pc.ValorSolicitado,
       pc.ValorTotalBem,
       CASE 
           WHEN COUNT(p.Status) FILTER (WHERE p.Status = 'Vencido') > 0 THEN 'ruim'
           ELSE 'bom'
       END AS Classe
FROM clientes c
JOIN PedidoCredito pc ON c.ClienteID = pc.ClienteID
JOIN ProdutosFinanciados pf ON pc.ProdutoID = pf.ProdutoID
LEFT JOIN ParcelasCredito p ON pc.SolicitacaoID = p.SolicitacaoID
WHERE pc.Status = 'Aprovado'
GROUP BY c.ClienteID, pf.NomeComercial, pc.ValorSolicitado, pc.ValorTotalBem
''' 
----------

Código: modelcreation_modified.py

import pandas as pd
from datetime import datetime
import numpy as np
import random as python_random
import joblib

from sklearn.preprocessing import StandardScaler,LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
import tensorflow as tf
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.regularizers import l2
from tensorflow.keras.layers import Dropout
from tensorflow.keras.callbacks import EarlyStopping

from utils import *

seed = 41
np.random.seed(seed)
python_random.seed(seed)
tf.random.set_seed(seed)

df = fetch_data_from_db(const.consulta_sql)

df['idade'] = df['idade'].astype(int)
df['valorsolicitado'] = df['valorsolicitado'].astype(float)
df['valortotalbem'] = df['valortotalbem'].astype(float)

substitui_nulos(df)

profissoes_validas = ['Advogado', 'Arquiteto', 'Cientista de Dados', 'Contador','Dentista','Empresário',
                 'Engenheiro','Médico','Programador']
corrigir_erros_digitacao(df, 'profissao', profissoes_validas)

df = tratar_outliers(df, 'tempoprofissao', 0, 70)
df = tratar_outliers(df, 'idade', 0, 110)

df['proporcaosolicitadototal'] = df['valorsolicitado'] / df['valortotalbem']
df['proporcaosolicitadototal'] = df['proporcaosolicitadototal'].astype(float)

X = df.drop('classe', axis=1)
y = df['classe']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)

X_test = save_scalers(X_test, ['tempoprofissao','renda','idade','dependentes','valorsolicitado','valortotalbem','proporcaosolicitadototal'])
X_train = save_scalers(X_train, ['tempoprofissao','renda','idade','dependentes','valorsolicitado','valortotalbem','proporcaosolicitadototal'])

X_test.head()

mapeamento = {'ruim': 0, 'bom': 1}
y_train = np.array([mapeamento[item] for item in y_train])
y_test = np.array([mapeamento[item] for item in y_test])

X_train = save_encoders(X_train, ['profissao', 'tiporesidencia', 'escolaridade','score','estadocivil','produto'])
X_test = save_encoders(X_test, ['profissao', 'tiporesidencia', 'escolaridade','score','estadocivil','produto'])

model = RandomForestClassifier()
selector = RFE(model, n_features_to_select=10, step=1)
selector = selector.fit(X_train, y_train)
X_train = selector.transform(X_train)
X_test = selector.transform(X_test)

joblib.dump(selector, 'selector.joblib')

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=l2(0.01), input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(64,kernel_regularizer=l2(0.01), activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32,kernel_regularizer=l2(0.01), activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(1,kernel_regularizer=l2(0.01), activation='sigmoid')
])
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

model.add(Dropout(0.5))
model.fit(
    X_train,
    y_train,
    callbacks=[EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)],    
    validation_split=0.2,  # Usa 20% dos dados para validação
    epochs=500,  # Número máximo de épocas
    batch_size=10,
    verbose=1
)

model.save('meu_modelo.keras')

y_pred = model.predict(X_test)
y_pred = (y_pred > 0.5).astype(int)  

print("Avaliação do Modelo nos Dados de Teste:")
model.evaluate(X_test, y_test)

print("\nRelatório de Classificação:")
print(classification_report(y_test, y_pred))

----------

Código: modelcreation.py

import pandas as pd
from datetime import datetime
import numpy as np
import random as python_random
import joblib

from sklearn.preprocessing import StandardScaler,LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
import tensorflow as tf

from utils import *
import const

#reprodutividade
seed = 41
np.random.seed(seed)
python_random.seed(seed)
tf.random.set_seed(seed)

#dados brutos
df = fetch_data_from_db(const.consulta_sql)

#conversão de tipo
df['idade'] = df['idade'].astype(int)
df['valorsolicitado'] = df['valorsolicitado'].astype(float)
df['valortotalbem'] = df['valortotalbem'].astype(float)

#Tratamento de Nulos
substitui_nulos(df)

#Trata Erros de Digitação
profissoes_validas = ['Advogado', 'Arquiteto', 'Cientista de Dados', 'Contador','Dentista','Empresário',
                                       'Engenheiro','Médico','Programador']
corrigir_erros_digitacao(df, 'profissao', profissoes_validas)

#Trata Outliers
df = tratar_outliers(df, 'tempoprofissao', 0, 70)
df = tratar_outliers(df, 'idade', 0, 110)

#Feature Engineering
df['proporcaosolicitadototal'] = df['valorsolicitado'] / df['valortotalbem']
df['proporcaosolicitadototal'] = df['proporcaosolicitadototal'].astype(float)

# Dividindo Dados
X = df.drop('classe', axis=1)
y = df['classe']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, 
                                                    random_state=seed)
# Normalização
X_test = save_scalers(X_test, ['tempoprofissao','renda','idade',
                               'dependentes','valorsolicitado','valortotalbem','proporcaosolicitadototal'])
X_train = save_scalers(X_train, ['tempoprofissao','renda','idade',
                                 'dependentes','valorsolicitado','valortotalbem','proporcaosolicitadototal'])

#Codificação
mapeamento = {'ruim': 0, 'bom': 1}
y_train = np.array([mapeamento[item] for item in y_train])
y_test = np.array([mapeamento[item] for item in y_test])
X_train = save_encoders(X_train, ['profissao', 'tiporesidencia', 
                                  'escolaridade','score','estadocivil','produto'])
X_test = save_encoders(X_test, ['profissao', 'tiporesidencia', 
                                'escolaridade','score','estadocivil','produto'])


# Seleção de Atributos
model = RandomForestClassifier()
# Instancia o RFE
selector = RFE(model, n_features_to_select=10, step=1)
selector = selector.fit(X_train, y_train)
# Transforma os dados
X_train = selector.transform(X_train)
X_test = selector.transform(X_test)
joblib.dump(selector, './objects/selector.joblib')

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
# Configurando o otimizador
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
# Compilando o modelo
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Treinamento do modelo 
model.fit(
    X_train,
    y_train,
    validation_split=0.2,  # Usa 20% dos dados para validação
    epochs=500,  # Número máximo de épocas
    batch_size=10,
    verbose=1
)
model.save('meu_modelo.keras')

# Previsões
y_pred = model.predict(X_test)
y_pred = (y_pred > 0.5).astype(int)  

# Avaliando o modelo
print("Avaliação do Modelo nos Dados de Teste:")
model.evaluate(X_test, y_test)

# Métricas de classificação
print("\nRelatório de Classificação:")
print(classification_report(y_test, y_pred))

----------

Código: requirements.txt

fuzzywuzzy==0.18.0
pandas==2.2.1
scikit-learn==1.4.1.post1
joblib==1.3.2
PyYAML==6.0.1
psycopg2-binary==2.9.9
numpy==1.26.4
tensorflow==2.16.1
shap==0.45.0
flask==3.0.3
python-Levenshtein==0.25.1
streamlit==1.32.1
matplotlib==3.8.3
seaborn==0.13.2

----------

Código: testflask.py

import requests
import yaml

with open('config.yaml', 'r') as file:
    config = yaml.safe_load(file)
    url = config['url_api']['url']

dados_novos = {
    'profissao': ['Advogado','Médico','Dentista','Contador'],
    'tempoprofissao': [39, 37, 16, 0],
    'renda': [20860.0, 5000, 20000, 7000],
    'tiporesidencia': ['Alugada','Própria','Própria','Alugada'],
    'escolaridade': ['Ens.Fundamental','PósouMais','Superior','Ens.Fundamental'],
    'score': ['Baixo','Baixo','MuitoBom','MuitoBom'],
    'idade': [36, 25, 19, 24],
    'dependentes': [0, 0, 4, 2],
    'estadocivil': ['Víuvo','Casado','Casado','Solteiro'],
    'produto': ['DoubleDuty','SpeedFury','ElegantCruise','TrailConqueror'],
    'valorsolicitado': [139244.0, 100000, 50000, 200000],
    'valortotalbem': [320000.0, 200000, 200000, 300000],
    'proporcaosolicitadototal': [2.2, 50, 200, 40]
}

response = requests.post(url, json=dados_novos)

if response.status_code == 200:
    print("Previsões recebidas:")
    predictions = response.json()
    print(predictions)
else:
    print("Erro ao fazer a previsão:", response.status_code)
                

----------

Código: utils.py

from fuzzywuzzy import process
import pandas as pd
from sklearn.preprocessing import StandardScaler,LabelEncoder
import joblib
import yaml
import psycopg2
import const

def fetch_data_from_db(sql_query):
    try:
        with open('config.yaml', 'r') as file:
            config = yaml.safe_load(file)

        con = psycopg2.connect(
            dbname=config['database_config']['dbname'], 
            user=config['database_config']['user'], 
            password=config['database_config']['password'], 
            host=config['database_config']['host']
        )

        cursor = con.cursor()
        cursor.execute(sql_query)

        df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])

    finally:
        if 'cursor' in locals():
            cursor.close()
        if 'con' in locals():
            con.close()

    return df

def substitui_nulos(df):
    for coluna in df.columns:
        if df[coluna].dtype == 'object':
            moda = df[coluna].mode()[0]
            df[coluna].fillna(moda, inplace=True)
        else:
            mediana = df[coluna].median()
            df[coluna].fillna(mediana, inplace=True)

def corrigir_erros_digitacao(df, coluna, lista_valida):
    for i, valor in enumerate(df[coluna]):
        valor_str = str(valor) if pd.notnull(valor) else valor

        if valor_str not in lista_valida and pd.notnull(valor_str):
            correcao = process.extractOne(valor_str, lista_valida)[0]
            df.at[i, coluna] = correcao

def tratar_outliers(df, coluna, minimo, maximo):
    mediana = df[(df[coluna] >= minimo) & (df[coluna] <= maximo)][coluna].median()
    df[coluna] = df[coluna].apply(lambda x: mediana if x < minimo or x > maximo else x)
    return df

def save_scalers(df, nome_colunas):
    for nome_coluna in nome_colunas:
        scaler = StandardScaler()
        df[nome_coluna] = scaler.fit_transform(df[[nome_coluna]])
        joblib.dump(scaler, f"./objects/scaler{nome_coluna}.joblib")

    return df

def save_encoders(df, nome_colunas):
    for nome_coluna in nome_colunas:
        label_encoder = LabelEncoder()
        df[nome_coluna] = label_encoder.fit_transform(df[nome_coluna])
        joblib.dump(label_encoder, f"./objects/labelencoder{nome_coluna}.joblib")

    return df

def load_scalers(df, nome_colunas):
    for nome_coluna in nome_colunas:
        nome_arquivo_scaler = f"./objects/scaler{nome_coluna}.joblib"
        scaler = joblib.load(nome_arquivo_scaler)
        df[nome_coluna] = scaler.transform(df[[nome_coluna]])
    return df

def load_encoders(df, nome_colunas):
    for nome_coluna in nome_colunas:
        nome_arquivo_encoder = f"./objects/labelencoder{nome_coluna}.joblib"
        label_encoder = joblib.load(nome_arquivo_encoder)
        df[nome_coluna] = label_encoder.transform(df[nome_coluna])
    return df            
                                                                    
----------

Código: Webapp.py

import streamlit as st
import requests
import yaml

st.title('Avaliação de Crédito')

with open('config.yaml', 'r') as file:
    config = yaml.safe_load(file)
    url = config['url_api']['url']

profissoes = ['Advogado', 'Arquiteto', 'Cientista de Dados', 
              'Contador', 'Dentista', 'Empresário', 'Engenheiro', 'Médico', 'Programador']
tipos_residencia = ['Alugada', 'Outros', 'Própria']
escolaridades = ['Ens.Fundamental', 'Ens.Médio', 'PósouMais', 'Superior']
scores = ['Baixo', 'Bom', 'Justo', 'MuitoBom']
estados_civis = ['Casado', 'Divorciado', 'Solteiro', 'Víuvo']
produtos = ['AgileXplorer', 'DoubleDuty', 'EcoPrestige', 'ElegantCruise', 
            'SpeedFury', 'TrailConqueror', 'VoyageRoamer', 'WorkMaster']


with st.form(key='prediction_form'):
    profissao = st.selectbox('Profissão', profissoes)
    tempo_profissao = st.number_input('Tempo na profissão (em anos)', min_value=0, value=0, step=1)
    renda = st.number_input('Renda mensal', min_value=0.0, value=0.0, step=1000.0)
    tipo_residencia = st.selectbox('Tipo de residência', tipos_residencia)
    escolaridade = st.selectbox('Escolaridade', escolaridades)
    score = st.selectbox('Score', scores)
    idade = st.number_input('Idade', min_value=18, max_value=110, value=25, step=1)
    dependentes = st.number_input('Dependentes', min_value=0, value=0, step=1)
    estado_civil = st.selectbox('Estado Civil', estados_civis)
    produto = st.selectbox('Produto', produtos)
    valor_solicitado = st.number_input('Valor solicitado', min_value=0.0, 
                                       value=0.0, step=1000.0)
    valor_total_bem = st.number_input('Valor total do bem', min_value=0.0, 
                                      value=0.0, step=1000.0)

    submit_button = st.form_submit_button(label='Consultar')

if submit_button:
    dados_novos = {
        'profissao': [profissao],
        'tempoprofissao': [tempo_profissao],
        'renda': [renda],
        'tiporesidencia': [tipo_residencia],
        'escolaridade': [escolaridade],
        'score': [score],
        'idade': [idade],
        'dependentes': [dependentes],
        'estadocivil': [estado_civil],
        'produto': [produto],
        'valorsolicitado': [valor_solicitado],
        'valortotalbem': [valor_total_bem],
        'proporcaosolicitadototal': [valor_total_bem / valor_solicitado]
    }

    
    response = requests.post(url, json=dados_novos)
    if response.status_code == 200:
        predictions = response.json()
        probabilidade = predictions[0][0] * 100  # Convertendo para porcentagem
        classe = "Bom" if probabilidade > 50 else "Ruim"
        st.success(f"Probabilidade: {probabilidade:.2f}%")
        st.success(f"Classe: {classe}")
    else:
        st.error(f"Erro ao fazer a previsão: {response.status_code}")
        
----------

Código: xai.py

import pandas as pd
from datetime import datetime
import numpy as np
import random as python_random
import joblib

from sklearn.preprocessing import StandardScaler,LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
import tensorflow as tf

from utils import *
import const

#reprodutividade
seed = 41
np.random.seed(seed)
python_random.seed(seed)
tf.random.set_seed(seed)

#dados brutos
df = fetch_data_from_db(const.consulta_sql)

#conversão de tipo
df['idade'] = df['idade'].astype(int)
df['valorsolicitado'] = df['valorsolicitado'].astype(float)
df['valortotalbem'] = df['valortotalbem'].astype(float)

#Tratamento de Nulos
substitui_nulos(df)

#Trata Erros de Digitação
profissoes_validas = ['Advogado', 'Arquiteto', 'Cientista de Dados', 'Contador','Dentista','Empresário',
                                       'Engenheiro','Médico','Programador']
corrigir_erros_digitacao(df, 'profissao', profissoes_validas)

#Trata Outliers
df = tratar_outliers(df, 'tempoprofissao', 0, 70)
df = tratar_outliers(df, 'idade', 0, 110)

#Feature Engineering
df['proporcaosolicitadototal'] = df['valorsolicitado'] / df['valortotalbem']
df['proporcaosolicitadototal'] = df['proporcaosolicitadototal'].astype(float)

# Dividindo Dados
X = df.drop('classe', axis=1)
y = df['classe']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, 
                                                    random_state=seed)
# Normalização
X_test = save_scalers(X_test, ['tempoprofissao','renda','idade',
                               'dependentes','valorsolicitado','valortotalbem','proporcaosolicitadototal'])
X_train = save_scalers(X_train, ['tempoprofissao','renda','idade',
                                 'dependentes','valorsolicitado','valortotalbem','proporcaosolicitadototal'])

#Codificação
mapeamento = {'ruim': 0, 'bom': 1}
y_train = np.array([mapeamento[item] for item in y_train])
y_test = np.array([mapeamento[item] for item in y_test])
X_train = save_encoders(X_train, ['profissao', 'tiporesidencia', 
                                  'escolaridade','score','estadocivil','produto'])
X_test = save_encoders(X_test, ['profissao', 'tiporesidencia', 
                                'escolaridade','score','estadocivil','produto'])

'''
# Seleção de Atributos
model = RandomForestClassifier()
# Instancia o RFE
selector = RFE(model, n_features_to_select=10, step=1)
selector = selector.fit(X_train, y_train)
# Transforma os dados
X_train = selector.transform(X_train)
X_test = selector.transform(X_test)
joblib.dump(selector, './objects/selector.joblib')'''

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
# Configurando o otimizador
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
# Compilando o modelo
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Treinamento do modelo 
model.fit(
    X_train,
    y_train,
    validation_split=0.2,  # Usa 20% dos dados para validação
    epochs=500,  # Número máximo de épocas
    batch_size=10,
    verbose=1
)
model.save('meu_modelo.keras')

# Previsões
y_pred = model.predict(X_test)
y_pred = (y_pred > 0.5).astype(int)  

# Avaliando o modelo
print("Avaliação do Modelo nos Dados de Teste:")
model.evaluate(X_test, y_test)

# Métricas de classificação
print("\nRelatório de Classificação:")
print(classification_report(y_test, y_pred))

# Função de Previsão Ajustada para LIME
def model_predict(data_asarray):
    data_asframe = pd.DataFrame(data_asarray, columns=X_train.columns)
    data_asframe = save_scalers(data_asframe, ['tempoprofissao', 'renda', 'idade', 'dependentes', 'valorsolicitado', 'valortotalbem', 'proporcaosolicitadototal'])
    data_asframe = save_encoders(data_asframe, ['profissao', 'tiporesidencia', 'escolaridade', 'score', 'estadocivil', 'produto'])
    predictions = model.predict(data_asframe)
    return np.hstack((1-predictions, predictions))

import lime
import lime.lime_tabular

# cria explainer
explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, 
                feature_names=X_train.columns, class_names=['ruim', 'bom'], mode='classification')
exp = explainer.explain_instance(X_test.values[1], model_predict, num_features=10)
#gera html
exp.save_to_file('lime_explanation.html')

print('\nImprimindo os recursos e seus pesos para Bom')
if 1 in exp.local_exp:
    for feature, weight in exp.local_exp[1]:
        print(f"{feature}: {weight}")

print("\nAcessar os valores das features e seus pesos para Bom")
feature_importances = exp.as_list(label=1)
for feature, weight in feature_importances:
    print(f"{feature}: {weight}")
        
----------

8 - Deploy
- Etapas
-- Criar Repositório no Github
-- Configurar gitignore
-- Fazer push para repositório Remoto
-- Configurar instancia do EC2
-- Fazer clone do Repositório
-- Rodar API e aplicação WEB
-- Testar com aplicação Local
-- Testar com aplicação Remota

8.1 - Instancia no EC2
- VM no AWS

8.2 - Ajuste no config.yaml

----------

Código: api.py

from flask import Flask, request, jsonify
from tensorflow.keras.models import load_model
import pandas as pd
import joblib
from utils import *

app = Flask(__name__)

model = load_model('meu_modelo.keras')
selector_carregado = joblib.load('./objects/selector.joblib')

@app.route('/predict', methods=['POST'])
def predict():
    input_data = request.get_json()

    df = pd.DataFrame(input_data)

    df = load_scalers(df, ['tempoprofissao', 'renda', 'idade', 'dependentes', 
                           'valorsolicitado', 'valortotalbem', 'proporcaosolicitadototal'])
    df = load_encoders(df, ['profissao', 'tiporesidencia', 'escolaridade', 'score', 
                            'estadocivil', 'produto'])
    df = selector_carregado.transform(df)

    predictions = model.predict(df)

    return jsonify(predictions.tolist())


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True) #Liberar requisições AWS

----------

8.3 - Criando o repositório e fazendo push
- Criar repositório Github
- Procedimento padrão

8.4 - Criando o arquivo gitignore
config.yaml # Arquivo de senha não pode ser enviado!!!
meu_modelo.keras
objects/*
!objects/.gitkeep
.gitignore
__pycache__
novadrivebank

8.5 - Configuração do repositório
- Acessar diretório no terminal VS CODE
cd D:\Github\NOME_DO_DIRETORIO

- Adicionar arquivos para sincronização Github
git add .                                                                                                 

- Commit dos arquivos a serem sinconizados
git commit -m "update commit"          

- Informar o novo diretório do github para sincronização    
git remote add origin https://github.com/alanjoffre/nome_diretorio_github.git

- Confirmar sincronização Github
git push -u origin master

8.6 - Criando VM AWS
- Faça login e acesse a pagina inicial Do AWS
- Pesquise sobre: EC2
- Na aba da esquerda, procure e clique no campo: Instances
- Selecione o campo: Launch as Instance
- Para o campo: Name and tags, informe: Novadrivebankvm
- Para o campo Application and OS Images (Amazon Machine Image), selecione a imagem: Ubuntu Server 22.04 LTS (HVM), SSD Volume Type
- Architecture: 64-bit
- Instance type: t2.medium
- Crie um novo par de chave para login
- Informe um nome para a sua par de chave
- Selecione em Key pair type: RSA
- Private key file formato: .pem
- Clique em: Create key pair
- Salve a chave em um local, para uso posterior.
- Selecione no campo: Key pair (login), a chave gerada na tela anterior.
- No campo: Network settings, deixe selecionado: Create security group para Firewall (security groups)
- Deixe marcado: Allow SSH traffic from, Allow HTTPS traffic from the internet e Allow HTTP traffic from the internet  
- Clique em: Launch instance, para que a instancia seja criada.
- Volte para aba: Instances e atualize a pagina
- Aguarde 2 min para que a instancia fique com o status: rodando...
- Clique em: Instance ID da instancia desejada
- Uma janela será aberta com as informações da instancia, desça a página e procure por: Security
- Clique no campo: Security groups
- Uma janela será aberta, no campo: Inbounds rules, clique em: Edit inbounds rules
- Clique em: Add rule
- Para o campo: Type, selecione: Custom TCP.
- Para o campo: Port range, informe 5000, que é a porta do Flask    
- Para o campo: Source, selecione: Anywhere-IPv4
- Crie outra regra
- Para o campo: Type, selecione: Custom TCP.
- Para o campo: Port range, informe 8501-8505, que é a porta do Streamlit    
- Para o campo: Source, selecione: Anywhere-IPv4
- Clique em: Save rules
- Volte para a tela principal da sua instancia
- Clique no botão: Connect
- Clique na aba: SSH client
- Copie o comando do campo: Exemple, para acessar o diretório remotamente de sua instancia na AWS por SSH
- Abra o prompt de comando do Windows, em sua máquina local
- Acesse o diretório onde foi copiada a chave para acesso a sua instancia no AWS.
- Copie o comando para acesso da sua instancia AWS, cole no prompt, dentro do diretório onde encontra-se sua chave
- Confirme a operação com: Yes
- Com acesso a console de sua instancia. digite: sudo apt update
- Abra o Github, no repositorio do seu projeto, clique no campo: Code, copie a url do seu projeto (HTTPS)
- Volte para a máquina virtual e digite: git clone URL_COPIADA_DO_GITHUB_REPOSITORIO
- Aperte a tecla ENTER
- Digite: ls para listar e verificar se o diretório do repositorio Github, encontra-se na instancia AWS
- Digite: cd novadrivebank/, para acessar o diretório
- Digite: python3 --version, para verificar a versão do python
- Crie o arquivo config.yaml
- Digite: touch config.yaml
- Digite: ls, para listar o diretório
- Digite: nano config.yaml, para editar o arquivo

----------
database_config:
  dbname: 'novadrivebank'
  user: 'etlreadonlybank'
  password: 'novadrive376A@'
  host: '159.223.187.110'

url_api:
   url: 'http://127.0.0.1:5000/predict'
   #url: 'http://ec2-44-210-140-14.compute-1.amazonaws.com:5000/predict'
----------

- Digite: Control + X
- Digite: y, depois Enter para salvar a edição
- Para ter certeza que foi salvo, digite: cat config.yaml

----------
database_config:
  dbname: 'novadrivebank'
  user: 'etlreadonlybank'
  password: 'novadrive376A@'
  host: '159.223.187.110'

url_api:
   url: 'http://127.0.0.1:5000/predict'
   #url: 'http://ec2-44-210-140-14.compute-1.amazonaws.com:5000/predict'
----------

- Instale o pip na console de sua instancia, basta digitar: sudo apt install python3-pip
- Selecione Y e dê ENTER
- Será aberta uma janela de configuração, clique na tecla TAB, não será alterado nada
- Continuará selecionado: polkit.service, cliquem em OK com a tecla ENTER
- Digite: ls para listar o diretório
- Instale os: requirements.txt
- Digite: pip3 install -r requirements.txt
- Crie os objetos do modelo, digite: python3 modelcreation.py
- Para que a console do terminal da instancia não fique travada/bloqueada, pois temos que rodar duas aplicações, digite o seguinte comando: nohup python3 api.py &
- A API já está rodando por padrão na porta: 5000
- Comando para verificar se a aplicação está rodando: sudo lsof -i : 5000
- Digite para rodar a aplicação streamlit: nohup python3 -m streamlit run webapp.py &
- Volte para a página AWS, instancia da aplicação, procure pelo campo: Public IPv4 DNS e copie o endereço
- Cole o endereço em uma nova aba do seu navegador e adicione: :8501 (dois pontos e 8501)
- Pronto, aplicação rodando totalmente online.

8.7 - Testando API (rodando na AWS)
- Acesse: config.yaml no VSCode

----------

Código: config.yaml pasta API

database_config:
  dbname: 'novadrivebank'
  user: 'etlreadonlybank'
  password: 'novadrive376A@'
  host: '159.223.187.110'

url_api:
   #url: 'http://127.0.0.1:5000/predict'
   url: 'http://ec2-44-210-140-14.compute-1.amazonaws.com:5000/predict'
   
----------   

- Ative o ambiente virtual no VSCode
- Digite: python .\testflask.py
- Tem que aparecer as previsões, caso apareça informações, estas, são enviadas pela instancia EC2-AWS 
- Digite: python -m streamlit run .\webapp.py
- Aplicação rodando remotamente



8.8 - Parando e terminando instancia no EC2/AWS - !!!Cuidado!!!
- Não deixe rodando instancia no AWS
-- Gera custos

- Para uma instancia
-- Ela continua no servidor, mas não gera custos

- Terminar uma instancia
-- Ela não constará mais no servidor, será excluída, não gera custos

9 - Considerações finais e possíveis melhorias

9.1 - Tratar labels não treinados
- Mesmo tratando o input(não selecionado na amostra)
- Como mitigar
-- Técnica de amostragem que garanta a presença dos labels no ntreino
-- Data Augmentation
-- Continual Learning
-- Novel Detection

9.2 - Técnicas de CI/CD
- Git Action
- Jenkins
- Ferramentas Especializadas no ciclo de vida do Modelo:
-- Mlflow
-- Sagemaker (AWS)
-- Azure Machine Learning

9.3 - Segurança / Autenticação
- Aplicação WEB
- API/FLASK

9.4 - Testes Unitários
- Pytest




